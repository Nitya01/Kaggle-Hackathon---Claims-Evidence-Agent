{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199f7eef",
   "metadata": {
    "_cell_guid": "fa2114c3-87a0-4721-b370-a952460a2d0c",
    "_uuid": "cb51b758-e7db-4671-bdd6-a32d07ace2a0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:19.066838Z",
     "iopub.status.busy": "2025-12-01T09:05:19.066423Z",
     "iopub.status.idle": "2025-12-01T09:05:39.821200Z",
     "shell.execute_reply": "2025-12-01T09:05:39.819677Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 20.76602,
     "end_time": "2025-12-01T09:05:39.823243",
     "exception": false,
     "start_time": "2025-12-01T09:05:19.057223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\r\n",
      "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\r\n",
      "cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\r\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mEnvironment setup complete\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -q -U google-generativeai scikit-learn pandas\n",
    "\n",
    "import os\n",
    "from google.colab import userdata # If on Colab\n",
    "from kaggle_secrets import UserSecretsClient # If on Kaggle\n",
    "\n",
    "# Setup API Key\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    os.environ[\"GEMINI_API_KEY\"] = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
    "except:\n",
    "    # Fallback for local/colab\n",
    "    # os.environ[\"GEMINI_API_KEY\"] = \"YOUR_RAW_KEY_HERE\" is unsafe, better to use input()\n",
    "    pass\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b52ab0",
   "metadata": {
    "_cell_guid": "0c063d1b-bef9-433c-8106-8280ab4f9909",
    "_uuid": "8b625d82-c3d0-46d7-acc1-e6d3c4d70d76",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:39.840445Z",
     "iopub.status.busy": "2025-12-01T09:05:39.840102Z",
     "iopub.status.idle": "2025-12-01T09:05:47.047531Z",
     "shell.execute_reply": "2025-12-01T09:05:47.046408Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 7.217992,
     "end_time": "2025-12-01T09:05:47.049376",
     "exception": false,
     "start_time": "2025-12-01T09:05:39.831384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI Version: 0.8.5\n",
      "Pandas Version: 2.3.3\n",
      "Scikit-Learn Version: 1.7.2\n",
      "Environment is ready!\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "print(f\"GenAI Version: {genai.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Scikit-Learn Version: {sklearn.__version__}\")\n",
    "print(\"Environment is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de680aa6",
   "metadata": {
    "_cell_guid": "f511b801-8d21-427e-afb9-8c1331b35951",
    "_uuid": "66270af1-09f7-487f-974d-a123d8f00e01",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007573,
     "end_time": "2025-12-01T09:05:47.064070",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.056497",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e18a72f",
   "metadata": {
    "_cell_guid": "b8e312c8-8b40-44d3-afd2-97efd1601221",
    "_uuid": "d31c1655-bf4b-4f5d-a899-3e77de7c254a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:47.081694Z",
     "iopub.status.busy": "2025-12-01T09:05:47.081105Z",
     "iopub.status.idle": "2025-12-01T09:05:47.189914Z",
     "shell.execute_reply": "2025-12-01T09:05:47.188642Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.120028,
     "end_time": "2025-12-01T09:05:47.191821",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.071793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qcb_capstone_data.zip'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, random, csv, textwrap, math, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(7)\n",
    "\n",
    "base = Path(\"/kaggle/working/qcb_evidence_agent/data\")\n",
    "app_dir = base / \"appendices\"\n",
    "claims_dir = base / \"claims\"\n",
    "base.mkdir(exist_ok=True, parents=True)\n",
    "app_dir.mkdir(exist_ok=True, parents=True)\n",
    "claims_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "themes = {\n",
    "    \"Summer School\": {\"prefix\": \"A\"},\n",
    "    \"Mentoring\": {\"prefix\": \"E\"},\n",
    "    \"Curriculum Modules\": {\"prefix\": \"C\"},\n",
    "    \"Research Engagement\": {\"prefix\": \"R\"},\n",
    "    \"Public Outreach\": {\"prefix\": \"P\"},\n",
    "    \"Collaboration Analysis\": {\"prefix\": \"M\"},\n",
    "}\n",
    "\n",
    "years = [2023, 2024, 2025]\n",
    "\n",
    "appendix_catalog = []\n",
    "\n",
    "def wrap(text, width=78):\n",
    "    return \"\\n\".join(textwrap.wrap(text, width=width))\n",
    "\n",
    "def write_survey_appendix(idx, theme):\n",
    "    \"\"\"Quant-style appendix with multi-year table\"\"\"\n",
    "    prefix = themes[theme][\"prefix\"]\n",
    "    app_id = f\"{prefix}{idx}\"\n",
    "    # choose 1–3 years\n",
    "    num_years = random.choice([1, 2, 3])\n",
    "    ys = sorted(random.sample(years, num_years))\n",
    "    lines = []\n",
    "    lines.append(f\"APPENDIX {app_id}. {theme.upper()} SURVEY SUMMARY, {ys[0]}–{ys[-1]}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Program Area: {theme}\")\n",
    "    lines.append(f\"Years Covered: {', '.join(str(y) for y in ys)}\")\n",
    "    lines.append(\"This appendix summarizes synthetic survey results on satisfaction and confidence.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Table {app_id}.1. Mean Satisfaction (1–5 scale) by Year\")\n",
    "    lines.append(\"\")\n",
    "    header = \"+--------+-------+-------+------+-----------------+\"\n",
    "    lines.append(header)\n",
    "    lines.append(\"| Year   |   N   | Mean  |  SD  |  % High (4–5)   |\")\n",
    "    lines.append(header)\n",
    "    stats_for_years = []\n",
    "    for y in ys:\n",
    "        n = random.randint(40, 160)\n",
    "        mean = round(random.uniform(3.3, 4.6), 2)\n",
    "        sd = round(random.uniform(0.4, 0.9), 2)\n",
    "        pct_high = round(random.uniform(55, 95), 1)\n",
    "        lines.append(f\"| {y:<6}|{n:6d} | {mean:4.2f} | {sd:4.2f} | {pct_high:7.1f}%        |\")\n",
    "        stats_for_years.append({\"year\": y, \"n\": n, \"mean\": mean, \"pct_high\": pct_high})\n",
    "    lines.append(header)\n",
    "    lines.append(\"\")\n",
    "    trend_note = \"increased\" if stats_for_years[-1][\"mean\"] >= stats_for_years[0][\"mean\"] else \"decreased\"\n",
    "    lines.append(\"Interpretive Notes:\")\n",
    "    lines.append(\n",
    "        wrap(\n",
    "            f\"Across {len(ys)} synthetic cohorts, average satisfaction {trend_note} from \"\n",
    "            f\"{stats_for_years[0]['mean']:.2f} in {stats_for_years[0]['year']} to \"\n",
    "            f\"{stats_for_years[-1]['mean']:.2f} in {stats_for_years[-1]['year']}. \"\n",
    "            \"Values are internally consistent but represent illustrative data only.\"\n",
    "        )\n",
    "    )\n",
    "    content = \"\\n\".join(lines)\n",
    "    (app_dir / f\"Appendix_{app_id}_{theme.replace(' ', '')}_Survey.txt\").write_text(content)\n",
    "    appendix_catalog.append({\n",
    "        \"app_id\": app_id,\n",
    "        \"theme\": theme,\n",
    "        \"type\": \"survey\",\n",
    "        \"years\": ys,\n",
    "        \"stats\": stats_for_years\n",
    "    })\n",
    "\n",
    "def write_transcript_appendix(idx, theme):\n",
    "    prefix = themes[theme][\"prefix\"]\n",
    "    app_id = f\"{prefix}{idx}\"\n",
    "    year = random.choice(years)\n",
    "    participants = [\"PhD1\", \"PhD2\", \"UG1\", \"UG2\", \"PI1\", \"Postdoc1\"]\n",
    "    chosen = random.sample(participants, k=random.randint(3, 5))\n",
    "    all_themes = [\n",
    "        \"Interdisciplinarity\", \"Instructor Clarity\", \"Pacing\",\n",
    "        \"Lab–Computation Connection\", \"Mentoring Support\",\n",
    "        \"Collaboration Barriers\", \"Belonging\", \"Cognitive Overload\"\n",
    "    ]\n",
    "    used_themes = random.sample(all_themes, k=random.randint(3, 5))\n",
    "    lines = []\n",
    "    lines.append(f\"APPENDIX {app_id}. {theme} Focus Group Transcript, {year}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Participants: {', '.join(chosen)}\")\n",
    "    lines.append(\"Length: synthetic 58 minutes\")\n",
    "    lines.append(\"\")\n",
    "    t = 0\n",
    "    for _ in range(20):\n",
    "        speaker = random.choice(chosen)\n",
    "        t += random.randint(1, 4)\n",
    "        mm = t // 60\n",
    "        ss = t % 60\n",
    "        ts = f\"{mm:02d}:{ss:02d}\"\n",
    "        comment_theme = random.choice(used_themes)\n",
    "        snippet = (f\"{speaker}: I felt that the {theme.lower()} experience in {year} \"\n",
    "                   f\"really shaped how I think about {comment_theme.lower()}.\")\n",
    "        lines.append(f\"{ts} {snippet}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Thematic Codes:\")\n",
    "    for th in used_themes:\n",
    "        subcodes = [f\"{th} – Depth\", f\"{th} – Barriers\"]\n",
    "        lines.append(f\"[Theme: {th}]\")\n",
    "        for sc in subcodes:\n",
    "            lines.append(f\"[Code: {sc}]\")\n",
    "    content = \"\\n\".join(lines)\n",
    "    (app_dir / f\"Appendix_{app_id}_{theme.replace(' ', '')}_Transcript.txt\").write_text(content)\n",
    "    appendix_catalog.append({\n",
    "        \"app_id\": app_id,\n",
    "        \"theme\": theme,\n",
    "        \"type\": \"transcript\",\n",
    "        \"years\": [year],\n",
    "        \"themes\": used_themes\n",
    "    })\n",
    "\n",
    "def write_observation_appendix(idx, theme):\n",
    "    prefix = themes[theme][\"prefix\"]\n",
    "    app_id = f\"{prefix}{idx}\"\n",
    "    year = random.choice(years)\n",
    "    session_title = random.choice([\n",
    "        \"Mechanistic Modeling\", \"Network Dynamics\", \"Microscopy Lab\",\n",
    "        \"Stochastic Processes\", \"Mentoring Roundtable\"\n",
    "    ])\n",
    "    lines = []\n",
    "    lines.append(f\"APPENDIX {app_id}. Observation Notes — {session_title}, {year}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Observer: Synthetic Evaluator\")\n",
    "    lines.append(\"Mode: In-person, structured observation\")\n",
    "    lines.append(\"\")\n",
    "    t = 0\n",
    "    for _ in range(12):\n",
    "        t += random.randint(2, 5)\n",
    "        mm = t // 60\n",
    "        ss = t % 60\n",
    "        ts = f\"{mm:02d}:{ss:02d}\"\n",
    "        note = random.choice([\n",
    "            \"Instructor checks in with groups; several participants ask for clarification.\",\n",
    "            \"Participants appear deeply engaged in small-group discussion.\",\n",
    "            \"A subset of the room seems confused by the notation on the board.\",\n",
    "            \"The instructor explicitly connects the computation to the experimental context.\",\n",
    "            \"Multiple students volunteer to share their reasoning at the board.\"\n",
    "        ])\n",
    "        lines.append(f\"{ts} {note}\")\n",
    "    lines.append(\"\")\n",
    "    clarity = random.randint(3, 5)\n",
    "    pacing = random.randint(2, 5)\n",
    "    engagement = random.randint(3, 5)\n",
    "    lines.append(\"Rubric Scores (1–5):\")\n",
    "    lines.append(f\"  Instructor Clarity: {clarity}\")\n",
    "    lines.append(f\"  Pacing: {pacing}\")\n",
    "    lines.append(f\"  Interactive Engagement: {engagement}\")\n",
    "    content = \"\\n\".join(lines)\n",
    "    (app_dir / f\"Appendix_{app_id}_{theme.replace(' ', '')}_Observation.txt\").write_text(content)\n",
    "    appendix_catalog.append({\n",
    "        \"app_id\": app_id,\n",
    "        \"theme\": theme,\n",
    "        \"type\": \"observation\",\n",
    "        \"years\": [year],\n",
    "        \"rubric\": {\"clarity\": clarity, \"pacing\": pacing, \"engagement\": engagement}\n",
    "    })\n",
    "\n",
    "def write_curriculum_appendix(idx):\n",
    "    theme = \"Curriculum Modules\"\n",
    "    prefix = themes[theme][\"prefix\"]\n",
    "    app_id = f\"{prefix}{idx}\"\n",
    "    year = random.choice(years)\n",
    "    module_name = random.choice([\"Modeling Module A\", \"Analysis Module B\", \"Imaging Module C\"])\n",
    "    lines = []\n",
    "    lines.append(f\"APPENDIX {app_id}. Curriculum Module Summary — {module_name}, {year}\")\n",
    "    lines.append(\"\")\n",
    "    los = [\n",
    "        \"Interpret basic mechanistic models.\",\n",
    "        \"Relate computational outputs to biological phenomena.\",\n",
    "        \"Collaborate across disciplinary backgrounds.\"\n",
    "    ]\n",
    "    lines.append(\"Learning Outcomes:\")\n",
    "    for lo in los:\n",
    "        lines.append(f\"  - {lo}\")\n",
    "    lines.append(\"\")\n",
    "    header = \"+--------+-------+-------+------+-----------------+\"\n",
    "    lines.append(f\"Table {app_id}.1. Student Ratings of the Module (1–5 scale)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(header)\n",
    "    lines.append(\"| Cohort |   N   | Mean  |  SD  |  % High (4–5)   |\")\n",
    "    lines.append(header)\n",
    "    cohorts = [\"2024 Pilot\", \"2025 Main\"]\n",
    "    stats = []\n",
    "    for c in cohorts:\n",
    "        n = random.randint(18, 60)\n",
    "        mean = round(random.uniform(3.4, 4.7), 2)\n",
    "        sd = round(random.uniform(0.4, 0.9), 2)\n",
    "        pct_high = round(random.uniform(60, 96), 1)\n",
    "        lines.append(f\"| {c:<8}|{n:6d} | {mean:4.2f} | {sd:4.2f} | {pct_high:7.1f}%        |\")\n",
    "        stats.append({\"cohort\": c, \"n\": n, \"mean\": mean, \"pct_high\": pct_high})\n",
    "    lines.append(header)\n",
    "    content = \"\\n\".join(lines)\n",
    "    (app_dir / f\"Appendix_{app_id}_CurriculumModule.txt\").write_text(content)\n",
    "    appendix_catalog.append({\n",
    "        \"app_id\": app_id,\n",
    "        \"theme\": theme,\n",
    "        \"type\": \"curriculum\",\n",
    "        \"years\": [year],\n",
    "        \"stats\": stats\n",
    "    })\n",
    "\n",
    "def write_research_engagement_appendix(idx):\n",
    "    theme = \"Research Engagement\"\n",
    "    prefix = themes[theme][\"prefix\"]\n",
    "    app_id = f\"{prefix}{idx}\"\n",
    "    ys = sorted(random.sample(years, k=random.choice([2, 3])))\n",
    "    lines = []\n",
    "    lines.append(f\"APPENDIX {app_id}. Research Engagement Hours, {ys[0]}–{ys[-1]}\")\n",
    "    lines.append(\"\")\n",
    "    header = \"+--------+----------------------+--------+-----------+\"\n",
    "    lines.append(header)\n",
    "    lines.append(\"| Year   | Activity Type        |  N     | Mean Hrs  |\")\n",
    "    lines.append(header)\n",
    "    activity_types = [\"Lab Work\", \"Mentor Meetings\", \"Group Analysis\", \"Workshops\"]\n",
    "    stats = []\n",
    "    for y in ys:\n",
    "        for a in activity_types:\n",
    "            n = random.randint(15, 70)\n",
    "            mean_hrs = round(random.uniform(4, 25), 1)\n",
    "            lines.append(f\"| {y:<6}| {a:<20} |{n:7d}| {mean_hrs:7.1f} |\")\n",
    "            stats.append({\"year\": y, \"activity\": a, \"n\": n, \"mean_hrs\": mean_hrs})\n",
    "    lines.append(header)\n",
    "    content = \"\\n\".join(lines)\n",
    "    (app_dir / f\"Appendix_{app_id}_ResearchEngagement.txt\").write_text(content)\n",
    "    appendix_catalog.append({\n",
    "        \"app_id\": app_id,\n",
    "        \"theme\": theme,\n",
    "        \"type\": \"research\",\n",
    "        \"years\": ys,\n",
    "        \"stats\": stats\n",
    "    })\n",
    "\n",
    "def write_collaboration_appendix(idx):\n",
    "    theme = \"Collaboration Analysis\"\n",
    "    prefix = themes[theme][\"prefix\"]\n",
    "    app_id = f\"{prefix}{idx}\"\n",
    "    year = random.choice(years)\n",
    "    nodes = random.randint(25, 60)\n",
    "    edges = random.randint(nodes, nodes * 4)\n",
    "    density = round((2*edges)/(nodes*(nodes-1)), 3)\n",
    "    avg_degree = round(2*edges/nodes, 2)\n",
    "    modularity = round(random.uniform(0.2, 0.6), 2)\n",
    "    communities = random.randint(3, 6)\n",
    "    lines = []\n",
    "    lines.append(f\"APPENDIX {app_id}. Collaboration Network Analysis, {year}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Network Size (nodes): {nodes}\")\n",
    "    lines.append(f\"Number of Edges: {edges}\")\n",
    "    lines.append(f\"Density: {density}\")\n",
    "    lines.append(f\"Average Degree: {avg_degree}\")\n",
    "    lines.append(f\"Modularity (Louvain): {modularity}\")\n",
    "    lines.append(f\"Communities Detected: {communities}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Interpretive Summary:\")\n",
    "    lines.append(wrap(\n",
    "        \"This synthetic collaboration network shows moderate modularity, indicating several \"\n",
    "        \"tightly connected sub-communities alongside cross-cutting ties. Values are illustrative.\"\n",
    "    ))\n",
    "    lines.append(\"\")\n",
    "    # small adjacency matrix snippet\n",
    "    k = min(8, nodes)\n",
    "    lines.append(\"Adjacency Matrix Snippet (first few nodes):\")\n",
    "    header = \"    \" + \" \".join(f\"{i:02d}\" for i in range(1, k+1))\n",
    "    lines.append(header)\n",
    "    for i in range(1, k+1):\n",
    "        row = [str(random.choice([0, 1])) for _ in range(k)]\n",
    "        lines.append(f\"{i:02d}  \" + \" \".join(row))\n",
    "    content = \"\\n\".join(lines)\n",
    "    (app_dir / f\"Appendix_{app_id}_CollaborationNetwork.txt\").write_text(content)\n",
    "    appendix_catalog.append({\n",
    "        \"app_id\": app_id,\n",
    "        \"theme\": theme,\n",
    "        \"type\": \"collaboration\",\n",
    "        \"years\": [year],\n",
    "        \"network\": {\n",
    "            \"nodes\": nodes,\n",
    "            \"edges\": edges,\n",
    "            \"density\": density,\n",
    "            \"avg_degree\": avg_degree,\n",
    "            \"modularity\": modularity,\n",
    "            \"communities\": communities\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Generate appendices according to a richer distribution\n",
    "# approx counts per domain\n",
    "counts_plan = {\n",
    "    \"survey\": 18,\n",
    "    \"transcript\": 15,\n",
    "    \"observation\": 10,\n",
    "    \"curriculum\": 10,\n",
    "    \"research\": 10,\n",
    "    \"collaboration\": 12\n",
    "}\n",
    "\n",
    "# For surveys: spread across core themes\n",
    "survey_themes = [\"Summer School\", \"Mentoring\", \"Curriculum Modules\", \"Research Engagement\", \"Public Outreach\"]\n",
    "si = {t: 0 for t in survey_themes}\n",
    "for _ in range(counts_plan[\"survey\"]):\n",
    "    t = random.choice(survey_themes)\n",
    "    si[t] += 1\n",
    "    write_survey_appendix(si[t], t)\n",
    "\n",
    "# Transcripts: mostly Summer, Mentoring, Curriculum\n",
    "transcript_themes = [\"Summer School\", \"Mentoring\", \"Curriculum Modules\"]\n",
    "ti = {t: 0 for t in transcript_themes}\n",
    "for _ in range(counts_plan[\"transcript\"]):\n",
    "    t = random.choice(transcript_themes)\n",
    "    ti[t] += 1\n",
    "    write_transcript_appendix(ti[t], t)\n",
    "\n",
    "# Observations: mix of Summer, Curriculum, Public Outreach\n",
    "obs_themes = [\"Summer School\", \"Curriculum Modules\", \"Public Outreach\"]\n",
    "oi = {t: 0 for t in obs_themes}\n",
    "for _ in range(counts_plan[\"observation\"]):\n",
    "    t = random.choice(obs_themes)\n",
    "    oi[t] += 1\n",
    "    write_observation_appendix(oi[t], t)\n",
    "\n",
    "# Curriculum module summaries\n",
    "for i in range(1, counts_plan[\"curriculum\"] + 1):\n",
    "    write_curriculum_appendix(i)\n",
    "\n",
    "# Research engagement\n",
    "for i in range(1, counts_plan[\"research\"] + 1):\n",
    "    write_research_engagement_appendix(i)\n",
    "\n",
    "# Collaboration analysis\n",
    "for i in range(1, counts_plan[\"collaboration\"] + 1):\n",
    "    write_collaboration_appendix(i)\n",
    "\n",
    "# Ensure we have about 75 appendices\n",
    "len_appendices = len(appendix_catalog)\n",
    "\n",
    "# Generate claims\n",
    "num_claims = 600\n",
    "incorrect_ratio = 0.20\n",
    "num_incorrect = int(num_claims * incorrect_ratio)\n",
    "\n",
    "claim_rows = []\n",
    "gt_rows = []\n",
    "\n",
    "def make_claim(app_meta, claim_id, correct: bool):\n",
    "    theme = app_meta[\"theme\"]\n",
    "    app_id = app_meta[\"app_id\"]\n",
    "    section_label = f\"Section 4.{random.randint(1,5)}\"\n",
    "    error_type = \"\"\n",
    "    claim_text = \"\"\n",
    "    detail = \"\"\n",
    "\n",
    "    if app_meta[\"type\"] == \"survey\":\n",
    "        ystat = random.choice(app_meta[\"stats\"])\n",
    "        y = ystat[\"year\"]\n",
    "        if correct:\n",
    "            claim_text = (\n",
    "                f\"In {y}, participants in the {theme.lower()} program reported an average \"\n",
    "                f\"satisfaction rating of approximately {ystat['mean']:.2f} on a 1–5 scale, \"\n",
    "                f\"with about {ystat['pct_high']:.1f}% selecting high values (4–5), as shown in Appendix {app_id}.\"\n",
    "            )\n",
    "        else:\n",
    "            wrong_mean = round(ystat[\"mean\"] + random.choice([-0.8, -0.5, 0.5, 0.8]), 2)\n",
    "            claim_text = (\n",
    "                f\"In {y}, satisfaction in the {theme.lower()} program dropped to around \"\n",
    "                f\"{wrong_mean:.2f} with fewer than 40% of participants choosing high values, \"\n",
    "                f\"according to Appendix {app_id}.\"\n",
    "            )\n",
    "            error_type = \"misreported_survey_stat\"\n",
    "        detail = f\"Table {app_id}.1\"\n",
    "\n",
    "    elif app_meta[\"type\"] == \"transcript\":\n",
    "        year = app_meta[\"years\"][0]\n",
    "        t_used = app_meta[\"themes\"]\n",
    "        all_possible = [\n",
    "            \"Interdisciplinarity\", \"Instructor Clarity\", \"Pacing\",\n",
    "            \"Lab–Computation Connection\", \"Mentoring Support\",\n",
    "            \"Collaboration Barriers\", \"Belonging\", \"Cognitive Overload\"\n",
    "        ]\n",
    "        if correct:\n",
    "            th = random.choice(t_used)\n",
    "            claim_text = (\n",
    "                f\"Focus group comments in Appendix {app_id} (from {year}) repeatedly highlight \"\n",
    "                f\"{th.lower()} as a salient theme in participants' experiences of the \"\n",
    "                f\"{theme.lower()} activities.\"\n",
    "            )\n",
    "        else:\n",
    "            th = random.choice([x for x in all_possible if x not in t_used] or all_possible)\n",
    "            claim_text = (\n",
    "                f\"Appendix {app_id} shows that {th.lower()} was the dominant recurring theme in \"\n",
    "                f\"participant discussions, with little attention paid to other issues.\"\n",
    "            )\n",
    "            error_type = \"qualitative_theme_mismatch\"\n",
    "        detail = \"Thematic codes section\"\n",
    "\n",
    "    elif app_meta[\"type\"] == \"observation\":\n",
    "        year = app_meta[\"years\"][0]\n",
    "        r = app_meta[\"rubric\"]\n",
    "        if correct:\n",
    "            claim_text = (\n",
    "                f\"Observation notes in Appendix {app_id} for {year} indicate relatively strong \"\n",
    "                f\"instructor clarity (rated {r['clarity']}/5) and high interactive engagement \"\n",
    "                f\"({r['engagement']}/5), with pacing rated at {r['pacing']}/5.\"\n",
    "            )\n",
    "        else:\n",
    "            claim_text = (\n",
    "                f\"Appendix {app_id} documents very low engagement scores (1/5) and uniformly \"\n",
    "                f\"poor clarity ratings for the observed {theme.lower()} session in {year}.\"\n",
    "            )\n",
    "            error_type = \"reversed_observation_ratings\"\n",
    "        detail = \"Rubric scores\"\n",
    "\n",
    "    elif app_meta[\"type\"] == \"curriculum\":\n",
    "        year = app_meta[\"years\"][0]\n",
    "        s = random.choice(app_meta[\"stats\"])\n",
    "        if correct:\n",
    "            claim_text = (\n",
    "                f\"For the {s['cohort']} cohort of the curriculum module documented in Appendix {app_id}, \"\n",
    "                f\"mean ratings were approximately {s['mean']:.2f} with about {s['pct_high']:.1f}% of students \"\n",
    "                f\"selecting high satisfaction (4–5).\"\n",
    "            )\n",
    "        else:\n",
    "            claim_text = (\n",
    "                f\"Appendix {app_id} shows that fewer than 30% of students in the {s['cohort']} cohort \"\n",
    "                f\"rated the module positively, with mean scores below 3.0.\"\n",
    "            )\n",
    "            error_type = \"fabricated_low_module_rating\"\n",
    "        detail = f\"Table {app_id}.1\"\n",
    "\n",
    "    elif app_meta[\"type\"] == \"research\":\n",
    "        s = random.choice(app_meta[\"stats\"])\n",
    "        if correct:\n",
    "            claim_text = (\n",
    "                f\"In {s['year']}, participants in the {theme.lower()} logs spent on average \"\n",
    "                f\"{s['mean_hrs']:.1f} hours on {s['activity'].lower()}, based on the synthetic data in Appendix {app_id}.\"\n",
    "            )\n",
    "        else:\n",
    "            claim_text = (\n",
    "                f\"Appendix {app_id} indicates that participants devoted less than 2 hours on average \"\n",
    "                f\"to {s['activity'].lower()} across all recorded years.\"\n",
    "            )\n",
    "            error_type = \"understated_research_hours\"\n",
    "        detail = f\"{s['year']} – {s['activity']}\"\n",
    "\n",
    "    elif app_meta[\"type\"] == \"collaboration\":\n",
    "        net = app_meta[\"network\"]\n",
    "        year = app_meta[\"years\"][0]\n",
    "        if correct:\n",
    "            claim_text = (\n",
    "                f\"The {year} collaboration network in Appendix {app_id} includes {net['nodes']} participants \"\n",
    "                f\"and {net['edges']} edges, with an average degree of about {net['avg_degree']:.2f} and \"\n",
    "                f\"modularity near {net['modularity']:.2f}, indicating several semi-independent communities.\"\n",
    "            )\n",
    "        else:\n",
    "            claim_text = (\n",
    "                f\"Appendix {app_id} describes a very sparse {year} collaboration network with fewer than \"\n",
    "                f\"10 edges and no discernible community structure (modularity near 0.0).\"\n",
    "            )\n",
    "            error_type = \"mischaracterized_network_structure\"\n",
    "        detail = \"Network summary\"\n",
    "\n",
    "    claim_rows.append({\n",
    "        \"claim_id\": claim_id,\n",
    "        \"section_label\": section_label,\n",
    "        \"theme\": theme,\n",
    "        \"claim_text\": claim_text,\n",
    "        \"is_correct\": str(correct),\n",
    "        \"source_appendices\": app_id,\n",
    "        \"source_details\": detail,\n",
    "        \"error_type\": error_type,\n",
    "    })\n",
    "    gt_rows.append({\n",
    "        \"claim_id\": claim_id,\n",
    "        \"source_appendices\": app_id,\n",
    "        \"source_tables\": detail,\n",
    "        \"source_lines\": app_meta[\"type\"],\n",
    "        \"is_correct\": str(correct),\n",
    "        \"error_type\": error_type,\n",
    "    })\n",
    "\n",
    "for i in range(num_claims):\n",
    "    cid = f\"C{i+1:03d}\"\n",
    "    correct = i >= num_incorrect  # first 20% incorrect\n",
    "    meta = random.choice(appendix_catalog)\n",
    "    make_claim(meta, cid, correct)\n",
    "\n",
    "# Write claims CSVs\n",
    "with open(claims_dir / \"claims_catalog.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=claim_rows[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(claim_rows)\n",
    "\n",
    "with open(claims_dir / \"claims_ground_truth.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=gt_rows[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(gt_rows)\n",
    "\n",
    "# Create a README description\n",
    "readme = \"\"\"\n",
    "QCB-Style Synthetic Evaluation Corpus\n",
    "=====================================\n",
    "\n",
    "This folder contains a synthetic, high-fidelity corpus designed to mimic a mixed-methods\n",
    "STEM center evaluation environment (similar in spirit to QCB), without using any real data.\n",
    "\n",
    "Structure\n",
    "---------\n",
    "- appendices/: 75 text-based appendices that look like exported PDF content.\n",
    "  These include:\n",
    "  * Survey-style summaries with multi-year tables and satisfaction metrics.\n",
    "  * Focus group and interview transcripts with full conversational turns, coded themes,\n",
    "    and subcodes.\n",
    "  * Lecture and workshop observation notes with timestamps and rubric scores.\n",
    "  * Curriculum module summaries with learning outcomes and module ratings.\n",
    "  * Research engagement logs with hours by activity type and year.\n",
    "  * Collaboration network summaries with graph statistics (nodes, edges, density,\n",
    "    modularity, communities) and small adjacency matrix snippets.\n",
    "\n",
    "- claims/:\n",
    "  * claims_catalog.csv – 600 synthetic claims that might appear in an evaluation\n",
    "    report. Around 20% are intentionally incorrect or misleading.\n",
    "  * claims_ground_truth.csv – a private mapping file indicating, for each claim:\n",
    "    - whether it is correct,\n",
    "    - which appendix it should be traced to,\n",
    "    - what kind of error is present if incorrect.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "This corpus is intended for building and evaluating an \"Evidence Tracing & Explanation Agent\"\n",
    "capstone project. An agent can:\n",
    "- Take a claim from claims_catalog.csv.\n",
    "- Retrieve and read the relevant appendix (and possibly others).\n",
    "- Decide whether the claim is supported by the evidence.\n",
    "- Produce a natural-language explanation pointing to the appropriate appendix and section.\n",
    "- Optionally, flag inconsistencies or missing evidence.\n",
    "\n",
    "All numbers, participants, and narratives are fabricated, but internally consistent\n",
    "within each appendix. Any resemblance to real programs or data is coincidental.\n",
    "\"\"\"\n",
    "(base / \"README_SYNTHETIC_DATA.txt\").write_text(readme.strip() + \"\\n\")\n",
    "\n",
    "# Zip it\n",
    "zip_path = shutil.make_archive(\"qcb_capstone_data\", \"zip\", base_dir=base)\n",
    "zip_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c89681",
   "metadata": {
    "_cell_guid": "71706f6b-1598-49cb-9c17-e04578dbb364",
    "_uuid": "e9f6c3b3-2712-435e-bee4-8c69566a0e7d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006513,
     "end_time": "2025-12-01T09:05:47.205364",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.198851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9fcd2d",
   "metadata": {
    "_cell_guid": "5276e026-be6c-4485-b75d-b4a11556f3db",
    "_uuid": "f4c791a6-e781-47b5-856b-d7ed7a89eca4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:47.220013Z",
     "iopub.status.busy": "2025-12-01T09:05:47.219647Z",
     "iopub.status.idle": "2025-12-01T09:05:47.226451Z",
     "shell.execute_reply": "2025-12-01T09:05:47.225497Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016166,
     "end_time": "2025-12-01T09:05:47.228074",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.211908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: /kaggle/working/qcb_evidence_agent/src\n",
      "Created: /kaggle/working/qcb_evidence_agent/data/appendices\n",
      "Created: /kaggle/working/qcb_evidence_agent/data/claims\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"/kaggle/working/qcb_evidence_agent\"\n",
    "dirs = [\n",
    "    f\"{base_dir}/src\",\n",
    "    f\"{base_dir}/data/appendices\",\n",
    "    f\"{base_dir}/data/claims\"\n",
    "]\n",
    "\n",
    "for d in dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    print(f\"Created: {d}\")\n",
    "\n",
    "# Create __init__.py so python can import from src\n",
    "with open(f\"{base_dir}/src/__init__.py\", \"w\") as f:\n",
    "    f.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9fc66",
   "metadata": {
    "_cell_guid": "7b58b869-d318-4131-aa25-eebd4d878c1b",
    "_uuid": "2b48b4ff-ac93-4e8f-b820-309ab313df45",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00706,
     "end_time": "2025-12-01T09:05:47.242097",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.235037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Implementing the RAG Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e880fbc8",
   "metadata": {
    "_cell_guid": "f1ff11fd-73aa-481f-8b87-f0b89b886fb5",
    "_uuid": "0267b2c0-b22a-4430-9f3e-af4e3159493f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:47.258248Z",
     "iopub.status.busy": "2025-12-01T09:05:47.257939Z",
     "iopub.status.idle": "2025-12-01T09:05:47.265227Z",
     "shell.execute_reply": "2025-12-01T09:05:47.263923Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017904,
     "end_time": "2025-12-01T09:05:47.266968",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.249064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/qcb_evidence_agent/src/data_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/qcb_evidence_agent/src/data_index.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class AppendixIndex:\n",
    "    def __init__(self, appendices_dir: str):\n",
    "        self.appendices_dir = Path(appendices_dir)\n",
    "        self.filepaths: List[Path] = []\n",
    "        self.texts: List[str] = []\n",
    "        self.appendix_ids: List[str] = []\n",
    "        self.vectorizer: TfidfVectorizer | None = None\n",
    "        self.tfidf_matrix = None\n",
    "\n",
    "    def load_appendices(self) -> None:\n",
    "        self.filepaths = sorted(self.appendices_dir.glob(\"*.txt\"))\n",
    "        self.texts = []\n",
    "        self.appendix_ids = []\n",
    "\n",
    "        for fp in self.filepaths:\n",
    "            text = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            self.texts.append(text)\n",
    "            stem = fp.stem\n",
    "            parts = stem.split(\"_\")\n",
    "            app_id = parts[1] if len(parts) > 1 else stem\n",
    "            self.appendix_ids.append(app_id)\n",
    "\n",
    "    def build_index(self) -> None:\n",
    "        self.vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.texts)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        if self.vectorizer is None:\n",
    "            raise RuntimeError(\"Index not built.\")\n",
    "\n",
    "        q_vec = self.vectorizer.transform([query])\n",
    "        scores = (self.tfidf_matrix @ q_vec.T).toarray().ravel()\n",
    "        top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_idx:\n",
    "            if scores[idx] <= 0: continue\n",
    "            full_text = self.texts[idx]\n",
    "            results.append({\n",
    "                \"appendix_id\": self.appendix_ids[idx],\n",
    "                \"filename\": str(self.filepaths[idx].name),\n",
    "                \"score\": float(scores[idx]),\n",
    "                \"snippet\": full_text[:800], # Increased snippet size for context\n",
    "                \"full_text\": full_text\n",
    "            })\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c8e0d6",
   "metadata": {
    "_cell_guid": "eb75a546-cbee-466f-a70a-72409593c391",
    "_uuid": "63f2253f-2895-4b1a-aaf8-a6510005e812",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:47.286091Z",
     "iopub.status.busy": "2025-12-01T09:05:47.285701Z",
     "iopub.status.idle": "2025-12-01T09:05:47.292528Z",
     "shell.execute_reply": "2025-12-01T09:05:47.291244Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019102,
     "end_time": "2025-12-01T09:05:47.294518",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.275416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/qcb_evidence_agent/src/tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/qcb_evidence_agent/src/tools.py\n",
    "from typing import List, Dict, Any\n",
    "from .data_index import AppendixIndex\n",
    "\n",
    "class AppendixSearchTool:\n",
    "    def __init__(self, appendices_dir: str):\n",
    "        self.index = AppendixIndex(appendices_dir)\n",
    "        self.index.load_appendices()\n",
    "        self.index.build_index()\n",
    "\n",
    "    def search_appendices(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        return self.index.search(query_text, top_k=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fc06a",
   "metadata": {
    "_cell_guid": "600900e6-cc96-47cb-9f37-861261bbf4de",
    "_uuid": "87b68954-ae53-4053-a9f0-5a36ff5c196d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00766,
     "end_time": "2025-12-01T09:05:47.309803",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.302143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Implement the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2870f5f7",
   "metadata": {
    "_cell_guid": "082c4383-6706-40e7-a8bd-dd71e4e168b1",
    "_uuid": "57885fc4-7033-46d0-946e-0b71e0220d19",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:47.326552Z",
     "iopub.status.busy": "2025-12-01T09:05:47.326264Z",
     "iopub.status.idle": "2025-12-01T09:05:47.334312Z",
     "shell.execute_reply": "2025-12-01T09:05:47.333314Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018766,
     "end_time": "2025-12-01T09:05:47.335910",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.317144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/qcb_evidence_agent/src/agents.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/qcb_evidence_agent/src/agents.py\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "import google.generativeai as genai\n",
    "from .tools import AppendixSearchTool\n",
    "\n",
    "def get_gemini_model(model_name: str = \"gemini-2.5-flash\") -> genai.GenerativeModel:\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\" CRITICAL WARNING: GEMINI_API_KEY is missing from environment variables!\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    return genai.GenerativeModel(\n",
    "        model_name, \n",
    "        generation_config={\"response_mime_type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "class ClaimUnderstandingAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def understand_claim(self, claim_id: str, claim_text: str) -> Dict[str, Any]:\n",
    "        prompt = f'''Analyze this claim. Extract JSON. Claim: \"{claim_text}\"\n",
    "        Output: {{ \"claim_id\": \"{claim_id}\", \"claim_text\": \"{claim_text}\", \"key_concepts\": [list of strings] }}'''\n",
    "        \n",
    "        try:\n",
    "            # Retry logic for rate limits\n",
    "            try:\n",
    "                resp = self.model.generate_content(prompt)\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    print(\"Rate limit hit. Waiting 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                    resp = self.model.generate_content(prompt)\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "            parsed = json.loads(resp.text)\n",
    "            if isinstance(parsed, list): parsed = parsed[0]\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            print(f\" ClaimUnderstandingAgent Error: {e}\")\n",
    "            return {\"claim_id\": claim_id, \"claim_text\": claim_text, \"key_concepts\": []}\n",
    "\n",
    "class EvidenceRetrievalAgent:\n",
    "    def __init__(self, search_tool):\n",
    "        self.search_tool = search_tool\n",
    "\n",
    "    def retrieve_evidence(self, parsed_claim: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        text = parsed_claim.get(\"claim_text\", \"\")\n",
    "        concepts = parsed_claim.get(\"key_concepts\", [])\n",
    "        if isinstance(concepts, list): concepts = \" \".join(str(c) for c in concepts)\n",
    "        query = f\"{text} {concepts}\"\n",
    "        \n",
    "        results = self.search_tool.search_appendices(query, top_k=4)\n",
    "        if not results:\n",
    "            print(f\" Warning: No evidence found for query: '{query[:50]}...'\")\n",
    "            \n",
    "        return {\"parsed_claim\": parsed_claim, \"candidate_evidence\": results}\n",
    "\n",
    "class EvidenceAnalysisAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def analyze(self, retrieval_bundle: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        claim = retrieval_bundle[\"parsed_claim\"]\n",
    "        evidence = retrieval_bundle[\"candidate_evidence\"]\n",
    "        \n",
    "        # Check if we actually have evidence\n",
    "        if not evidence:\n",
    "            return {\"verdict\": \"unsupported\", \"explanation\": \"No relevant evidence found in the appendices.\", \"evidence_appendix_ids\": []}\n",
    "\n",
    "        ev_text = \"\".join([f\"\\n[Appendix {e['appendix_id']}]: {e['snippet']}...\" for e in evidence])\n",
    "        \n",
    "        prompt = f'''Verify if claim is supported by evidence.\n",
    "        Claim: \"{claim.get('claim_text','')}\"\n",
    "        Evidence: {ev_text}\n",
    "        Output JSON: {{ \"verdict\": \"supported\"|\"unsupported\"|\"partially_supported\", \"evidence_appendix_ids\": [], \"explanation\": \"\" }}'''\n",
    "        \n",
    "        try:\n",
    "            try:\n",
    "                resp = self.model.generate_content(prompt)\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    print(\"Rate limit hit (Analysis). Waiting 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                    resp = self.model.generate_content(prompt)\n",
    "                else:\n",
    "                    raise e\n",
    "                    \n",
    "            parsed = json.loads(resp.text)\n",
    "            if isinstance(parsed, list): parsed = parsed[0]\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            print(f\"EvidenceAnalysisAgent Error: {e}\")\n",
    "            return {\"verdict\": \"error\", \"explanation\": f\"System Error: {str(e)}\", \"evidence_appendix_ids\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7b881",
   "metadata": {
    "_cell_guid": "43a1276b-1981-42ff-bf04-68b983136bd9",
    "_uuid": "ce43d2b7-e6a3-4632-bcef-ea6f51cb63a7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007459,
     "end_time": "2025-12-01T09:05:47.350380",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.342921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f03a4b8a",
   "metadata": {
    "_cell_guid": "e49bccdd-0878-4f66-8aa6-2e2238b25f07",
    "_uuid": "ea412f78-44e5-4891-8db6-87280ddd6dcb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:47.365525Z",
     "iopub.status.busy": "2025-12-01T09:05:47.365184Z",
     "iopub.status.idle": "2025-12-01T09:05:47.372857Z",
     "shell.execute_reply": "2025-12-01T09:05:47.371755Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017213,
     "end_time": "2025-12-01T09:05:47.374389",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.357176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/qcb_evidence_agent/src/orchestrator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/qcb_evidence_agent/src/orchestrator.py\n",
    "import pandas as pd\n",
    "from .agents import get_gemini_model, ClaimUnderstandingAgent, EvidenceRetrievalAgent, EvidenceAnalysisAgent\n",
    "from .tools import AppendixSearchTool\n",
    "\n",
    "class EvidenceTracingOrchestrator:\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.model = get_gemini_model()\n",
    "        \n",
    "        # Initialize Tools\n",
    "        self.search_tool = AppendixSearchTool(f\"{data_dir}/data/appendices\")\n",
    "        \n",
    "        # Initialize Agents\n",
    "        self.understander = ClaimUnderstandingAgent(self.model)\n",
    "        self.retriever = EvidenceRetrievalAgent(self.search_tool)\n",
    "        self.analyzer = EvidenceAnalysisAgent(self.model)\n",
    "        \n",
    "        # Load claims\n",
    "        self.claims_df = pd.read_csv(f\"{data_dir}/data/claims/claims_catalog.csv\")\n",
    "\n",
    "    def review_claim(self, claim_id: str):\n",
    "        # 0. Get Data\n",
    "        row = self.claims_df[self.claims_df[\"claim_id\"] == claim_id].iloc[0]\n",
    "        \n",
    "        # 1. Understand\n",
    "        parsed = self.understander.understand_claim(claim_id, row[\"claim_text\"])\n",
    "        \n",
    "        # 2. Retrieve\n",
    "        context = self.retriever.retrieve_evidence(parsed)\n",
    "        \n",
    "        # 3. Analyze\n",
    "        result = self.analyzer.analyze(context)\n",
    "        \n",
    "        return {\n",
    "            \"claim_id\": claim_id,\n",
    "            \"original_text\": row[\"claim_text\"],\n",
    "            \"verdict\": result[\"verdict\"],\n",
    "            \"explanation\": result[\"explanation\"],\n",
    "            \"cited_appendices\": result[\"evidence_appendix_ids\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee32e6b",
   "metadata": {
    "_cell_guid": "da945964-98c2-4e66-961e-0952d34eaae6",
    "_uuid": "988cb56d-944d-4dc4-8c77-712baa307e6e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007215,
     "end_time": "2025-12-01T09:05:47.389326",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.382111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Final Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed12615",
   "metadata": {
    "_cell_guid": "aee4bf40-09c8-4661-845e-e14f3fadd10b",
    "_uuid": "27766096-52ab-4a4d-aa07-0c3193fd589f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:47.405380Z",
     "iopub.status.busy": "2025-12-01T09:05:47.405022Z",
     "iopub.status.idle": "2025-12-01T09:05:47.543558Z",
     "shell.execute_reply": "2025-12-01T09:05:47.542348Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.149087,
     "end_time": "2025-12-01T09:05:47.545449",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.396362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key successfully loaded from Kaggle Secrets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Try to load from Kaggle Secrets first\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"GEMINI_API_KEY\") \n",
    "    os.environ[\"GEMINI_API_KEY\"] = api_key\n",
    "    print(\"✅ API Key successfully loaded from Kaggle Secrets.\")\n",
    "except Exception as e:\n",
    "    # Fallback: If secrets aren't set up, paste it manually\n",
    "    print(\"⚠️ Could not find Kaggle Secret 'GEMINI_API_KEY'.\")\n",
    "    print(\"Please paste your API Key below:\")\n",
    "    os.environ[\"GEMINI_API_KEY\"] = input(\"Enter your Gemini API Key: \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f0f6c04",
   "metadata": {
    "_cell_guid": "0d2e694c-8653-475b-88be-0ac0b120f17d",
    "_uuid": "b371ea50-f9ad-4030-abd3-796e93a09d26",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:47.561509Z",
     "iopub.status.busy": "2025-12-01T09:05:47.561186Z",
     "iopub.status.idle": "2025-12-01T09:05:52.510121Z",
     "shell.execute_reply": "2025-12-01T09:05:52.508996Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.959099,
     "end_time": "2025-12-01T09:05:52.511992",
     "exception": false,
     "start_time": "2025-12-01T09:05:47.552893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Orchestrator (Building Index)...\n",
      "\n",
      "--- Processing Claim C075 ---\n",
      "\n",
      "Claim: In 2023, satisfaction in the research engagement program dropped to around 3.95 with fewer than 40% of participants choosing high values, according to Appendix R3.\n",
      "Verdict: UNSUPPORTED\n",
      "Explanation: According to Appendix R3, in 2023, the mean satisfaction was 4.45, not around 3.95. Furthermore, 61.0% of participants chose high values (4-5), which is not fewer than 40%.\n",
      "Sources: ['R3']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Add source code to python path\n",
    "sys.path.append(\"/kaggle/working/qcb_evidence_agent\")\n",
    "\n",
    "from src.orchestrator import EvidenceTracingOrchestrator\n",
    "\n",
    "# Initialize the system\n",
    "print(\"Initializing Orchestrator (Building Index)...\")\n",
    "orchestrator = EvidenceTracingOrchestrator(\"/kaggle/working/qcb_evidence_agent\")\n",
    "\n",
    "# Pick a claim to test\n",
    "target_claim = \"C075\"\n",
    "\n",
    "print(f\"\\n--- Processing Claim {target_claim} ---\\n\")\n",
    "result = orchestrator.review_claim(target_claim)\n",
    "\n",
    "# Display Results\n",
    "print(f\"Claim: {result['original_text']}\")\n",
    "print(f\"Verdict: {result['verdict'].upper()}\")\n",
    "print(f\"Explanation: {result['explanation']}\")\n",
    "print(f\"Sources: {result['cited_appendices']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea81a713",
   "metadata": {
    "_cell_guid": "58442e3b-3d87-4084-ad8d-ce9a96bf4851",
    "_uuid": "51491607-0c67-4ef0-ba3b-2ed3ac400d9f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007123,
     "end_time": "2025-12-01T09:05:52.526950",
     "exception": false,
     "start_time": "2025-12-01T09:05:52.519827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77077c6c",
   "metadata": {
    "_cell_guid": "e3dcec9c-2c04-43c3-a1bc-bf254e3eaee9",
    "_uuid": "c27be8bd-d116-488f-b138-8af071c33599",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:52.544973Z",
     "iopub.status.busy": "2025-12-01T09:05:52.544669Z",
     "iopub.status.idle": "2025-12-01T09:05:52.552373Z",
     "shell.execute_reply": "2025-12-01T09:05:52.551334Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020086,
     "end_time": "2025-12-01T09:05:52.553973",
     "exception": false,
     "start_time": "2025-12-01T09:05:52.533887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/qcb_evidence_agent/src/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/qcb_evidence_agent/src/evaluation.py\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Dict, Any, List\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from .orchestrator import EvidenceTracingOrchestrator\n",
    "\n",
    "def normalize_verdict(v: str) -> str:\n",
    "    v = str(v).lower().strip()\n",
    "    if v in [\"supported\", \"true\", \"yes\"]:\n",
    "        return \"supported\"\n",
    "    if v in [\"unsupported\", \"false\", \"no\"]:\n",
    "        return \"unsupported\"\n",
    "    return \"partially_supported\"\n",
    "\n",
    "def normalize_gt(gt: str) -> str:\n",
    "    return \"supported\" if str(gt) == \"True\" else \"unsupported\"\n",
    "\n",
    "def run_batch_eval(data_dir: str, max_claims: int = 20) -> pd.DataFrame:\n",
    "    print(f\"Starting evaluation on {max_claims} claims...\")\n",
    "    \n",
    "    orch = EvidenceTracingOrchestrator(data_dir)\n",
    "    \n",
    "    claims_path = f\"{data_dir}/data/claims/claims_catalog.csv\"\n",
    "    gt_path = f\"{data_dir}/data/claims/claims_ground_truth.csv\"\n",
    "    \n",
    "    df_claims = pd.read_csv(claims_path)\n",
    "    df_gt = pd.read_csv(gt_path)\n",
    "    \n",
    "    # 1. Prepare Ground Truth\n",
    "    gt_subset = df_gt[['claim_id', 'is_correct']].copy()\n",
    "    gt_subset = gt_subset.rename(columns={'is_correct': 'gt_is_correct'})\n",
    "    \n",
    "    # 2. Merge\n",
    "    df_merged = pd.merge(df_claims, gt_subset, on='claim_id', how='inner')\n",
    "    \n",
    "    # 3. Sample\n",
    "    df_sample = df_merged.sample(n=min(max_claims, len(df_merged)), random_state=42)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df_sample.iterrows():\n",
    "        cid = row['claim_id']\n",
    "        text = row['claim_text']\n",
    "        gt_raw = row['gt_is_correct']\n",
    "        \n",
    "        try:\n",
    "            # Run Agent\n",
    "            prediction = orch.review_claim(cid)\n",
    "            pred_verdict = prediction.get('verdict', 'error')\n",
    "            \n",
    "            y_pred = normalize_verdict(pred_verdict)\n",
    "            y_true = normalize_gt(gt_raw)\n",
    "            \n",
    "            results.append({\n",
    "                \"claim_id\": cid,\n",
    "                \"claim_text\": text,\n",
    "                \"ground_truth_raw\": gt_raw,\n",
    "                \"agent_verdict_raw\": pred_verdict,\n",
    "                \"y_true\": y_true,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"explanation\": prediction.get('explanation', '')\n",
    "            })\n",
    "            print(f\"Processed {cid}: GT={y_true} | Pred={y_pred}\")\n",
    "            \n",
    "            # FIX: SLEEP TO RESPECT RATE LIMITS\n",
    "            time.sleep(10) \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {cid}: {e}\")\n",
    "            time.sleep(10) # Wait even on error to let quota reset\n",
    "            \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def print_report(df_results: pd.DataFrame):\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"EVALUATION REPORT\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if df_results.empty:\n",
    "        print(\"No results to show.\")\n",
    "        return\n",
    "\n",
    "    y_true = df_results['y_true']\n",
    "    y_pred = df_results['y_pred']\n",
    "    \n",
    "    valid_mask = y_pred != 'error'\n",
    "    \n",
    "    if valid_mask.sum() == 0:\n",
    "        print(\"No valid predictions found.\")\n",
    "        return\n",
    "\n",
    "    acc = accuracy_score(y_true[valid_mask], y_pred[valid_mask])\n",
    "    print(f\"Accuracy: {acc:.2%}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true[valid_mask], y_pred[valid_mask], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "061cc55b",
   "metadata": {
    "_cell_guid": "fcb25a19-8a1a-45f2-ae5c-1417297ec9cb",
    "_uuid": "08ac74a1-8e16-4bd4-a1af-a8e09ea7a837",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:05:52.570416Z",
     "iopub.status.busy": "2025-12-01T09:05:52.570081Z",
     "iopub.status.idle": "2025-12-01T09:11:03.009871Z",
     "shell.execute_reply": "2025-12-01T09:11:03.008904Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 310.458427,
     "end_time": "2025-12-01T09:11:03.019893",
     "exception": false,
     "start_time": "2025-12-01T09:05:52.561466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 20 claims...\n",
      "Processed C111: GT=unsupported | Pred=unsupported\n",
      "Processed C420: GT=supported | Pred=supported\n",
      "Processed C566: GT=supported | Pred=supported\n",
      "Processed C078: GT=unsupported | Pred=unsupported\n",
      "Processed C182: GT=supported | Pred=supported\n",
      "Processed C285: GT=supported | Pred=supported\n",
      "Processed C011: GT=unsupported | Pred=unsupported\n",
      "Processed C470: GT=supported | Pred=supported\n",
      "Processed C079: GT=unsupported | Pred=unsupported\n",
      "Processed C350: GT=supported | Pred=supported\n",
      "Processed C056: GT=unsupported | Pred=unsupported\n",
      "Processed C119: GT=unsupported | Pred=unsupported\n",
      "Processed C110: GT=unsupported | Pred=unsupported\n",
      "Processed C589: GT=supported | Pred=supported\n",
      "Processed C370: GT=supported | Pred=unsupported\n",
      "Processed C235: GT=supported | Pred=unsupported\n",
      "Processed C031: GT=unsupported | Pred=unsupported\n",
      "Processed C213: GT=supported | Pred=unsupported\n",
      "Processed C185: GT=supported | Pred=supported\n",
      "Processed C087: GT=unsupported | Pred=unsupported\n",
      "\n",
      "========================================\n",
      "EVALUATION REPORT\n",
      "========================================\n",
      "Accuracy: 85.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   supported       1.00      0.73      0.84        11\n",
      " unsupported       0.75      1.00      0.86         9\n",
      "\n",
      "    accuracy                           0.85        20\n",
      "   macro avg       0.88      0.86      0.85        20\n",
      "weighted avg       0.89      0.85      0.85        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append(\"/kaggle/working/qcb_evidence_agent\")\n",
    "\n",
    "# Force Reload\n",
    "import src.evaluation\n",
    "importlib.reload(src.evaluation)\n",
    "\n",
    "from src.evaluation import run_batch_eval, print_report\n",
    "\n",
    "# Run again\n",
    "df_results = run_batch_eval(\"/kaggle/working/qcb_evidence_agent\", max_claims=20)\n",
    "print_report(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c21fe",
   "metadata": {
    "_cell_guid": "f835660b-46d7-4136-9bed-b9ed555e5463",
    "_uuid": "c8ce8c83-0fcb-4a9b-933b-6df455b45d03",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.008018,
     "end_time": "2025-12-01T09:11:03.036134",
     "exception": false,
     "start_time": "2025-12-01T09:11:03.028116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Testing with Other Personalized Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e358ba12",
   "metadata": {
    "_cell_guid": "3a7312cf-1cfd-4335-867e-1a2f53dfc59c",
    "_uuid": "0d9a5c09-e6e3-4599-b5e5-19f093d935bc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:11:03.054208Z",
     "iopub.status.busy": "2025-12-01T09:11:03.053892Z",
     "iopub.status.idle": "2025-12-01T09:11:03.060935Z",
     "shell.execute_reply": "2025-12-01T09:11:03.059684Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018182,
     "end_time": "2025-12-01T09:11:03.062519",
     "exception": false,
     "start_time": "2025-12-01T09:11:03.044337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/qcb_evidence_agent/src/orchestrator_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/qcb_evidence_agent/src/orchestrator_2.py\n",
    "from typing import Dict, Any\n",
    "from .agents import get_gemini_model, ClaimUnderstandingAgent, EvidenceRetrievalAgent, EvidenceAnalysisAgent\n",
    "from .tools import AppendixSearchTool\n",
    "\n",
    "class ChatOrchestrator:\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.model = get_gemini_model()\n",
    "        \n",
    "        # Initialize Tools (Shared with the other orchestrator)\n",
    "        self.search_tool = AppendixSearchTool(f\"{data_dir}/data/appendices\")\n",
    "        \n",
    "        # Initialize Agents\n",
    "        self.understander = ClaimUnderstandingAgent(self.model)\n",
    "        self.retriever = EvidenceRetrievalAgent(self.search_tool)\n",
    "        self.analyzer = EvidenceAnalysisAgent(self.model)\n",
    "\n",
    "    def review_freeform_claim(self, claim_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Takes raw user input, runs the agent pipeline, and returns the result.\n",
    "        No CSV lookup required.\n",
    "        \"\"\"\n",
    "        # 1. Understand (Use 'user_query' as a dummy ID)\n",
    "        parsed = self.understander.understand_claim(\"user_query\", claim_text)\n",
    "        \n",
    "        # 2. Retrieve\n",
    "        context = self.retriever.retrieve_evidence(parsed)\n",
    "        \n",
    "        # 3. Analyze\n",
    "        result = self.analyzer.analyze(context)\n",
    "        \n",
    "        # 4. Return structured response for the UI\n",
    "        return {\n",
    "            \"original_text\": claim_text,\n",
    "            \"verdict\": result.get(\"verdict\", \"error\"),\n",
    "            \"explanation\": result.get(\"explanation\", \"No explanation provided.\"),\n",
    "            \"cited_appendices\": result.get(\"evidence_appendix_ids\", []),\n",
    "            \"evidence_snippets\": context.get(\"candidate_evidence\", []) \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c774416",
   "metadata": {
    "_cell_guid": "a51a165d-e024-4df9-ae98-3902b03049ee",
    "_uuid": "19816db9-645a-4c6e-9d5c-e89d4dad0f2a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:11:03.081017Z",
     "iopub.status.busy": "2025-12-01T09:11:03.080687Z",
     "iopub.status.idle": "2025-12-01T09:11:19.229135Z",
     "shell.execute_reply": "2025-12-01T09:11:19.227741Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 16.159669,
     "end_time": "2025-12-01T09:11:19.230984",
     "exception": false,
     "start_time": "2025-12-01T09:11:03.071315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chat Agent...\n",
      "\n",
      "User Query: In the 2023 mechanistic modeling session, the instructor frequently checked in with students and explicitly connected the computation to the experimental context, demonstrating consistently high instructor clarity\n",
      "----------------------------------------\n",
      "VERDICT: PARTIALLY_SUPPORTED\n",
      "REASONING: The evidence from Appendix A3 and Appendix A5, both pertaining to the 2023 Mechanistic Modeling session, indicates that the instructor did check in with groups and explicitly connected the computation to the experimental context multiple times. For example, in A3, the instructor checked in at 00:11 and connected computation to context at 00:16, 00:18, 00:27, and 00:30. In A5, checking in occurred at 00:18 and 00:28, and connecting computation to context occurred at 00:03, 00:23, and 00:30. These observations support the occurrence of the instructor's actions. However, the claim that this demonstrated 'consistently high instructor clarity' is contradicted by multiple entries within these same appendices, such as 'A subset of the room seems confused by the notation on the board' (e.g., A3 at 00:07, 00:20, 00:23; A5 at 00:10, 00:15, 00:20) and 'several participants ask for clarification' (e.g., A3 at 00:11; A5 at 00:18, 00:28). This suggests that clarity was not consistently high, despite the instructor's efforts to engage and connect the material.\n",
      "\n",
      "User Query: In the 2023 summer school, participants consistently described a strong sense of improved belonging throughout the program\n",
      "----------------------------------------\n",
      "VERDICT: UNSUPPORTED\n",
      "REASONING: The claim states that participants in the 2023 summer school consistently described a strong sense of improved belonging. However, the provided transcripts for the 2023 summer school (Appendix A4 and Appendix A7) do not contain any mentions of 'belonging' from participants. Appendix A6 mentions 'belonging' (00:17 UG2), but this is in the context of the 2024 summer school experience, not 2023. Therefore, there is no evidence to support the claim about the 2023 summer school.\n"
     ]
    }
   ],
   "source": [
    "### Personalized Text Claims Checking\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/working/qcb_evidence_agent\")\n",
    "\n",
    "from src.orchestrator_2 import ChatOrchestrator\n",
    "\n",
    "print(\"Initializing Chat Agent...\")\n",
    "chat_agent = ChatOrchestrator(\"/kaggle/working/qcb_evidence_agent\")\n",
    "\n",
    "# Test a custom query\n",
    "user_query = \"In the 2023 mechanistic modeling session, the instructor frequently checked in with students and explicitly connected the computation to the experimental context, demonstrating consistently high instructor clarity\"\n",
    "print(f\"\\nUser Query: {user_query}\")\n",
    "print(\"-\" * 40)\n",
    "result = chat_agent.review_freeform_claim(user_query)\n",
    "\n",
    "print(f\"VERDICT: {result['verdict'].upper()}\")\n",
    "print(f\"REASONING: {result['explanation']}\")\n",
    "\n",
    "# Test a custom query 2\n",
    "user_query_2 = \"In the 2023 summer school, participants consistently described a strong sense of improved belonging throughout the program\"\n",
    "print(f\"\\nUser Query: {user_query_2}\")\n",
    "print(\"-\" * 40)\n",
    "result_2 = chat_agent.review_freeform_claim(user_query_2)\n",
    "\n",
    "print(f\"VERDICT: {result_2['verdict'].upper()}\")\n",
    "print(f\"REASONING: {result_2['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e272333",
   "metadata": {
    "papermill": {
     "duration": 0.008359,
     "end_time": "2025-12-01T09:11:19.248719",
     "exception": false,
     "start_time": "2025-12-01T09:11:19.240360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### UI App Model (For Easier Navigation and Clear Presentation of Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4134e10f",
   "metadata": {
    "_cell_guid": "e95dd812-cfb5-4bb3-8abb-040755e23b7f",
    "_uuid": "ade25de2-0356-418e-bc73-d6755ce9266e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:11:19.267480Z",
     "iopub.status.busy": "2025-12-01T09:11:19.267175Z",
     "iopub.status.idle": "2025-12-01T09:11:19.274118Z",
     "shell.execute_reply": "2025-12-01T09:11:19.273061Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018476,
     "end_time": "2025-12-01T09:11:19.275662",
     "exception": false,
     "start_time": "2025-12-01T09:11:19.257186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/qcb_evidence_agent/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/qcb_evidence_agent/app.py\n",
    "import streamlit as st\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure src is in pythonpath\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import the new Chat Orchestrator\n",
    "from src.orchestrator_2 import ChatOrchestrator\n",
    "\n",
    "# Config\n",
    "DATA_DIR = \".\"\n",
    "st.set_page_config(page_title=\"Evidence Agent Chat\", layout=\"wide\")\n",
    "\n",
    "@st.cache_resource\n",
    "def get_agent():\n",
    "    # Load the chat-specific agent\n",
    "    return ChatOrchestrator(DATA_DIR)\n",
    "\n",
    "st.title(\"Evidence Tracing Chatbot\")\n",
    "st.markdown(\"Type any claim below, and I will check the synthetic evidence base for you.\")\n",
    "\n",
    "# Input Area\n",
    "claim_text = st.text_area(\"Enter your claim:\", height=100, placeholder=\"e.g., Student confidence in coding tasks improved significantly in 2024.\")\n",
    "\n",
    "if st.button(\"Verify Claim\"):\n",
    "    if not claim_text:\n",
    "        st.warning(\"Please enter some text first.\")\n",
    "    else:\n",
    "        agent = get_agent()\n",
    "        \n",
    "        with st.spinner(\"Analyzing evidence...\"):\n",
    "            # Call the freeform method\n",
    "            result = agent.review_freeform_claim(claim_text)\n",
    "        \n",
    "        # 1. Show Verdict\n",
    "        verdict = result['verdict'].upper()\n",
    "        \n",
    "        # Dynamic color coding\n",
    "        if \"SUPPORTED\" in verdict and \"PARTIALLY\" not in verdict:\n",
    "            color = \"green\"\n",
    "        elif \"UNSUPPORTED\" in verdict:\n",
    "            color = \"red\"\n",
    "        else:\n",
    "            color = \"orange\" # Partial or Error\n",
    "            \n",
    "        st.markdown(f\"### Verdict: :{color}[{verdict}]\")\n",
    "        \n",
    "        # 2. Show Explanation\n",
    "        st.markdown(f\"**Analysis:** {result['explanation']}\")\n",
    "        \n",
    "        # 3. Show Cited Evidence\n",
    "        if result['evidence_snippets']:\n",
    "            st.divider()\n",
    "            st.subheader(\"Evidence Retrieved\")\n",
    "            for ev in result['evidence_snippets']:\n",
    "                with st.expander(f\"Appendix {ev['appendix_id']} (Relevance: {ev['score']:.2f})\"):\n",
    "                    st.info(f\"Source File: {ev['filename']}\")\n",
    "                    st.text(ev['snippet'])\n",
    "        else:\n",
    "            st.info(\"No relevant evidence found in the appendices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96ac6cd2",
   "metadata": {
    "_cell_guid": "e36e2cd8-2425-478c-973b-6f4b76d5b246",
    "_uuid": "ed93852c-1d42-4543-8b64-001209079935",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:11:19.294334Z",
     "iopub.status.busy": "2025-12-01T09:11:19.294054Z",
     "iopub.status.idle": "2025-12-01T09:11:19.313216Z",
     "shell.execute_reply": "2025-12-01T09:11:19.311962Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030556,
     "end_time": "2025-12-01T09:11:19.314823",
     "exception": false,
     "start_time": "2025-12-01T09:11:19.284267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping data...\n",
      "Data unzipped successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# 1. Create the project skeleton\n",
    "base_dir = \"/kaggle/working/qcb_evidence_agent\"\n",
    "src_dir = f\"{base_dir}/src\"\n",
    "data_dir = f\"{base_dir}/data\"\n",
    "\n",
    "os.makedirs(src_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# 2. Unzip data \n",
    "zip_path = \"/kaggle/working/qcb_capstone_data.zip\" \n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    print(\"Unzipping data...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    print(\"Data unzipped successfully.\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find {zip_path}. Please ensure the file is uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "293185d9",
   "metadata": {
    "_cell_guid": "da0e4863-f534-4bff-aa78-70197cb103f9",
    "_uuid": "822351ee-00ff-491e-bccb-57d3b1e7b959",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T09:11:19.333921Z",
     "iopub.status.busy": "2025-12-01T09:11:19.333551Z",
     "iopub.status.idle": "2025-12-01T09:11:19.372458Z",
     "shell.execute_reply": "2025-12-01T09:11:19.371557Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.050638,
     "end_time": "2025-12-01T09:11:19.374165",
     "exception": false,
     "start_time": "2025-12-01T09:11:19.323527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click below to download your full project:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='evidence_agent_full_project.zip' target='_blank'>evidence_agent_full_project.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/evidence_agent_full_project.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Zip the entire project folder\n",
    "shutil.make_archive(\"/kaggle/working/evidence_agent_full_project\", 'zip', \"/kaggle/working/qcb_evidence_agent\")\n",
    "\n",
    "print(\"Click below to download your full project:\")\n",
    "display(FileLink('evidence_agent_full_project.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f705ca3",
   "metadata": {
    "_cell_guid": "1c008276-4e44-4e7f-bce0-b23cae3ef402",
    "_uuid": "c543e1bb-575b-4794-a834-bb8895ea3099",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.008692,
     "end_time": "2025-12-01T09:11:19.392345",
     "exception": false,
     "start_time": "2025-12-01T09:11:19.383653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f011d74",
   "metadata": {
    "_cell_guid": "88b377a1-31d2-4194-8b1c-a859fde894fe",
    "_uuid": "be21ea7a-a635-46fc-87ca-d7d5153077a4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.008536,
     "end_time": "2025-12-01T09:11:19.409566",
     "exception": false,
     "start_time": "2025-12-01T09:11:19.401030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 368.594763,
   "end_time": "2025-12-01T09:11:22.433280",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-01T09:05:13.838517",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
