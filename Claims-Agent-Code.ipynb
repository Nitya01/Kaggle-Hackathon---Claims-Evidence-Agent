{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary libraries\n!pip install -q -U google-generativeai scikit-learn pandas\n\nimport os\nfrom google.colab import userdata # If on Colab\nfrom kaggle_secrets import UserSecretsClient # If on Kaggle\n\n# Setup API Key\n# 1. Get your key from aistudio.google.com\n# 2. In Kaggle: Add-ons -> Secrets -> Add a new secret called 'GEMINI_API_KEY'\ntry:\n    user_secrets = UserSecretsClient()\n    os.environ[\"GEMINI_API_KEY\"] = user_secrets.get_secret(\"GEMINI_API_KEY\")\nexcept:\n    # Fallback for local/colab\n    # os.environ[\"GEMINI_API_KEY\"] = \"YOUR_RAW_KEY_HERE\" # Unsafe, better to use input()\n    pass\n\nprint(\"Environment setup complete\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:45.174512Z","iopub.execute_input":"2025-11-30T07:31:45.174750Z","iopub.status.idle":"2025-11-30T07:31:50.118175Z","shell.execute_reply.started":"2025-11-30T07:31:45.174730Z","shell.execute_reply":"2025-11-30T07:31:50.116705Z"}},"outputs":[{"name":"stdout","text":"Environment setup complete\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import google.generativeai as genai\nimport pandas as pd\nimport sklearn\n\nprint(f\"GenAI Version: {genai.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Scikit-Learn Version: {sklearn.__version__}\")\nprint(\"Environment is ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:50.119427Z","iopub.execute_input":"2025-11-30T07:31:50.119692Z","iopub.status.idle":"2025-11-30T07:31:50.125849Z","shell.execute_reply.started":"2025-11-30T07:31:50.119666Z","shell.execute_reply":"2025-11-30T07:31:50.124909Z"}},"outputs":[{"name":"stdout","text":"GenAI Version: 0.8.5\nPandas Version: 2.3.3\nScikit-Learn Version: 1.7.2\nEnvironment is ready!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Data Generation","metadata":{}},{"cell_type":"code","source":"import os, random, csv, textwrap, math, shutil\nfrom pathlib import Path\n\nrandom.seed(7)\n\nbase = Path(\"/kaggle/working/qcb_evidence_agent/data\")\napp_dir = base / \"appendices\"\nclaims_dir = base / \"claims\"\nbase.mkdir(exist_ok=True, parents=True)\napp_dir.mkdir(exist_ok=True, parents=True)\nclaims_dir.mkdir(exist_ok=True, parents=True)\n\nthemes = {\n    \"Summer School\": {\"prefix\": \"A\"},\n    \"Mentoring\": {\"prefix\": \"E\"},\n    \"Curriculum Modules\": {\"prefix\": \"C\"},\n    \"Research Engagement\": {\"prefix\": \"R\"},\n    \"Public Outreach\": {\"prefix\": \"P\"},\n    \"Collaboration Analysis\": {\"prefix\": \"M\"},\n}\n\nyears = [2023, 2024, 2025]\n\nappendix_catalog = []\n\ndef wrap(text, width=78):\n    return \"\\n\".join(textwrap.wrap(text, width=width))\n\ndef write_survey_appendix(idx, theme):\n    \"\"\"Quant-style appendix with multi-year table\"\"\"\n    prefix = themes[theme][\"prefix\"]\n    app_id = f\"{prefix}{idx}\"\n    # choose 1â€“3 years\n    num_years = random.choice([1, 2, 3])\n    ys = sorted(random.sample(years, num_years))\n    lines = []\n    lines.append(f\"APPENDIX {app_id}. {theme.upper()} SURVEY SUMMARY, {ys[0]}â€“{ys[-1]}\")\n    lines.append(\"\")\n    lines.append(f\"Program Area: {theme}\")\n    lines.append(f\"Years Covered: {', '.join(str(y) for y in ys)}\")\n    lines.append(\"This appendix summarizes synthetic survey results on satisfaction and confidence.\")\n    lines.append(\"\")\n    lines.append(f\"Table {app_id}.1. Mean Satisfaction (1â€“5 scale) by Year\")\n    lines.append(\"\")\n    header = \"+--------+-------+-------+------+-----------------+\"\n    lines.append(header)\n    lines.append(\"| Year   |   N   | Mean  |  SD  |  % High (4â€“5)   |\")\n    lines.append(header)\n    stats_for_years = []\n    for y in ys:\n        n = random.randint(40, 160)\n        mean = round(random.uniform(3.3, 4.6), 2)\n        sd = round(random.uniform(0.4, 0.9), 2)\n        pct_high = round(random.uniform(55, 95), 1)\n        lines.append(f\"| {y:<6}|{n:6d} | {mean:4.2f} | {sd:4.2f} | {pct_high:7.1f}%        |\")\n        stats_for_years.append({\"year\": y, \"n\": n, \"mean\": mean, \"pct_high\": pct_high})\n    lines.append(header)\n    lines.append(\"\")\n    trend_note = \"increased\" if stats_for_years[-1][\"mean\"] >= stats_for_years[0][\"mean\"] else \"decreased\"\n    lines.append(\"Interpretive Notes:\")\n    lines.append(\n        wrap(\n            f\"Across {len(ys)} synthetic cohorts, average satisfaction {trend_note} from \"\n            f\"{stats_for_years[0]['mean']:.2f} in {stats_for_years[0]['year']} to \"\n            f\"{stats_for_years[-1]['mean']:.2f} in {stats_for_years[-1]['year']}. \"\n            \"Values are internally consistent but represent illustrative data only.\"\n        )\n    )\n    content = \"\\n\".join(lines)\n    (app_dir / f\"Appendix_{app_id}_{theme.replace(' ', '')}_Survey.txt\").write_text(content)\n    appendix_catalog.append({\n        \"app_id\": app_id,\n        \"theme\": theme,\n        \"type\": \"survey\",\n        \"years\": ys,\n        \"stats\": stats_for_years\n    })\n\ndef write_transcript_appendix(idx, theme):\n    prefix = themes[theme][\"prefix\"]\n    app_id = f\"{prefix}{idx}\"\n    year = random.choice(years)\n    participants = [\"PhD1\", \"PhD2\", \"UG1\", \"UG2\", \"PI1\", \"Postdoc1\"]\n    chosen = random.sample(participants, k=random.randint(3, 5))\n    all_themes = [\n        \"Interdisciplinarity\", \"Instructor Clarity\", \"Pacing\",\n        \"Labâ€“Computation Connection\", \"Mentoring Support\",\n        \"Collaboration Barriers\", \"Belonging\", \"Cognitive Overload\"\n    ]\n    used_themes = random.sample(all_themes, k=random.randint(3, 5))\n    lines = []\n    lines.append(f\"APPENDIX {app_id}. {theme} Focus Group Transcript, {year}\")\n    lines.append(\"\")\n    lines.append(f\"Participants: {', '.join(chosen)}\")\n    lines.append(\"Length: synthetic 58 minutes\")\n    lines.append(\"\")\n    t = 0\n    for _ in range(20):\n        speaker = random.choice(chosen)\n        t += random.randint(1, 4)\n        mm = t // 60\n        ss = t % 60\n        ts = f\"{mm:02d}:{ss:02d}\"\n        comment_theme = random.choice(used_themes)\n        snippet = (f\"{speaker}: I felt that the {theme.lower()} experience in {year} \"\n                   f\"really shaped how I think about {comment_theme.lower()}.\")\n        lines.append(f\"{ts} {snippet}\")\n    lines.append(\"\")\n    lines.append(\"Thematic Codes:\")\n    for th in used_themes:\n        subcodes = [f\"{th} â€“ Depth\", f\"{th} â€“ Barriers\"]\n        lines.append(f\"[Theme: {th}]\")\n        for sc in subcodes:\n            lines.append(f\"[Code: {sc}]\")\n    content = \"\\n\".join(lines)\n    (app_dir / f\"Appendix_{app_id}_{theme.replace(' ', '')}_Transcript.txt\").write_text(content)\n    appendix_catalog.append({\n        \"app_id\": app_id,\n        \"theme\": theme,\n        \"type\": \"transcript\",\n        \"years\": [year],\n        \"themes\": used_themes\n    })\n\ndef write_observation_appendix(idx, theme):\n    prefix = themes[theme][\"prefix\"]\n    app_id = f\"{prefix}{idx}\"\n    year = random.choice(years)\n    session_title = random.choice([\n        \"Mechanistic Modeling\", \"Network Dynamics\", \"Microscopy Lab\",\n        \"Stochastic Processes\", \"Mentoring Roundtable\"\n    ])\n    lines = []\n    lines.append(f\"APPENDIX {app_id}. Observation Notes â€” {session_title}, {year}\")\n    lines.append(\"\")\n    lines.append(\"Observer: Synthetic Evaluator\")\n    lines.append(\"Mode: In-person, structured observation\")\n    lines.append(\"\")\n    t = 0\n    for _ in range(12):\n        t += random.randint(2, 5)\n        mm = t // 60\n        ss = t % 60\n        ts = f\"{mm:02d}:{ss:02d}\"\n        note = random.choice([\n            \"Instructor checks in with groups; several participants ask for clarification.\",\n            \"Participants appear deeply engaged in small-group discussion.\",\n            \"A subset of the room seems confused by the notation on the board.\",\n            \"The instructor explicitly connects the computation to the experimental context.\",\n            \"Multiple students volunteer to share their reasoning at the board.\"\n        ])\n        lines.append(f\"{ts} {note}\")\n    lines.append(\"\")\n    clarity = random.randint(3, 5)\n    pacing = random.randint(2, 5)\n    engagement = random.randint(3, 5)\n    lines.append(\"Rubric Scores (1â€“5):\")\n    lines.append(f\"  Instructor Clarity: {clarity}\")\n    lines.append(f\"  Pacing: {pacing}\")\n    lines.append(f\"  Interactive Engagement: {engagement}\")\n    content = \"\\n\".join(lines)\n    (app_dir / f\"Appendix_{app_id}_{theme.replace(' ', '')}_Observation.txt\").write_text(content)\n    appendix_catalog.append({\n        \"app_id\": app_id,\n        \"theme\": theme,\n        \"type\": \"observation\",\n        \"years\": [year],\n        \"rubric\": {\"clarity\": clarity, \"pacing\": pacing, \"engagement\": engagement}\n    })\n\ndef write_curriculum_appendix(idx):\n    theme = \"Curriculum Modules\"\n    prefix = themes[theme][\"prefix\"]\n    app_id = f\"{prefix}{idx}\"\n    year = random.choice(years)\n    module_name = random.choice([\"Modeling Module A\", \"Analysis Module B\", \"Imaging Module C\"])\n    lines = []\n    lines.append(f\"APPENDIX {app_id}. Curriculum Module Summary â€” {module_name}, {year}\")\n    lines.append(\"\")\n    los = [\n        \"Interpret basic mechanistic models.\",\n        \"Relate computational outputs to biological phenomena.\",\n        \"Collaborate across disciplinary backgrounds.\"\n    ]\n    lines.append(\"Learning Outcomes:\")\n    for lo in los:\n        lines.append(f\"  - {lo}\")\n    lines.append(\"\")\n    header = \"+--------+-------+-------+------+-----------------+\"\n    lines.append(f\"Table {app_id}.1. Student Ratings of the Module (1â€“5 scale)\")\n    lines.append(\"\")\n    lines.append(header)\n    lines.append(\"| Cohort |   N   | Mean  |  SD  |  % High (4â€“5)   |\")\n    lines.append(header)\n    cohorts = [\"2024 Pilot\", \"2025 Main\"]\n    stats = []\n    for c in cohorts:\n        n = random.randint(18, 60)\n        mean = round(random.uniform(3.4, 4.7), 2)\n        sd = round(random.uniform(0.4, 0.9), 2)\n        pct_high = round(random.uniform(60, 96), 1)\n        lines.append(f\"| {c:<8}|{n:6d} | {mean:4.2f} | {sd:4.2f} | {pct_high:7.1f}%        |\")\n        stats.append({\"cohort\": c, \"n\": n, \"mean\": mean, \"pct_high\": pct_high})\n    lines.append(header)\n    content = \"\\n\".join(lines)\n    (app_dir / f\"Appendix_{app_id}_CurriculumModule.txt\").write_text(content)\n    appendix_catalog.append({\n        \"app_id\": app_id,\n        \"theme\": theme,\n        \"type\": \"curriculum\",\n        \"years\": [year],\n        \"stats\": stats\n    })\n\ndef write_research_engagement_appendix(idx):\n    theme = \"Research Engagement\"\n    prefix = themes[theme][\"prefix\"]\n    app_id = f\"{prefix}{idx}\"\n    ys = sorted(random.sample(years, k=random.choice([2, 3])))\n    lines = []\n    lines.append(f\"APPENDIX {app_id}. Research Engagement Hours, {ys[0]}â€“{ys[-1]}\")\n    lines.append(\"\")\n    header = \"+--------+----------------------+--------+-----------+\"\n    lines.append(header)\n    lines.append(\"| Year   | Activity Type        |  N     | Mean Hrs  |\")\n    lines.append(header)\n    activity_types = [\"Lab Work\", \"Mentor Meetings\", \"Group Analysis\", \"Workshops\"]\n    stats = []\n    for y in ys:\n        for a in activity_types:\n            n = random.randint(15, 70)\n            mean_hrs = round(random.uniform(4, 25), 1)\n            lines.append(f\"| {y:<6}| {a:<20} |{n:7d}| {mean_hrs:7.1f} |\")\n            stats.append({\"year\": y, \"activity\": a, \"n\": n, \"mean_hrs\": mean_hrs})\n    lines.append(header)\n    content = \"\\n\".join(lines)\n    (app_dir / f\"Appendix_{app_id}_ResearchEngagement.txt\").write_text(content)\n    appendix_catalog.append({\n        \"app_id\": app_id,\n        \"theme\": theme,\n        \"type\": \"research\",\n        \"years\": ys,\n        \"stats\": stats\n    })\n\ndef write_collaboration_appendix(idx):\n    theme = \"Collaboration Analysis\"\n    prefix = themes[theme][\"prefix\"]\n    app_id = f\"{prefix}{idx}\"\n    year = random.choice(years)\n    nodes = random.randint(25, 60)\n    edges = random.randint(nodes, nodes * 4)\n    density = round((2*edges)/(nodes*(nodes-1)), 3)\n    avg_degree = round(2*edges/nodes, 2)\n    modularity = round(random.uniform(0.2, 0.6), 2)\n    communities = random.randint(3, 6)\n    lines = []\n    lines.append(f\"APPENDIX {app_id}. Collaboration Network Analysis, {year}\")\n    lines.append(\"\")\n    lines.append(f\"Network Size (nodes): {nodes}\")\n    lines.append(f\"Number of Edges: {edges}\")\n    lines.append(f\"Density: {density}\")\n    lines.append(f\"Average Degree: {avg_degree}\")\n    lines.append(f\"Modularity (Louvain): {modularity}\")\n    lines.append(f\"Communities Detected: {communities}\")\n    lines.append(\"\")\n    lines.append(\"Interpretive Summary:\")\n    lines.append(wrap(\n        \"This synthetic collaboration network shows moderate modularity, indicating several \"\n        \"tightly connected sub-communities alongside cross-cutting ties. Values are illustrative.\"\n    ))\n    lines.append(\"\")\n    # small adjacency matrix snippet\n    k = min(8, nodes)\n    lines.append(\"Adjacency Matrix Snippet (first few nodes):\")\n    header = \"    \" + \" \".join(f\"{i:02d}\" for i in range(1, k+1))\n    lines.append(header)\n    for i in range(1, k+1):\n        row = [str(random.choice([0, 1])) for _ in range(k)]\n        lines.append(f\"{i:02d}  \" + \" \".join(row))\n    content = \"\\n\".join(lines)\n    (app_dir / f\"Appendix_{app_id}_CollaborationNetwork.txt\").write_text(content)\n    appendix_catalog.append({\n        \"app_id\": app_id,\n        \"theme\": theme,\n        \"type\": \"collaboration\",\n        \"years\": [year],\n        \"network\": {\n            \"nodes\": nodes,\n            \"edges\": edges,\n            \"density\": density,\n            \"avg_degree\": avg_degree,\n            \"modularity\": modularity,\n            \"communities\": communities\n        }\n    })\n\n# Generate appendices according to a richer distribution\n# approx counts per domain\ncounts_plan = {\n    \"survey\": 18,\n    \"transcript\": 15,\n    \"observation\": 10,\n    \"curriculum\": 10,\n    \"research\": 10,\n    \"collaboration\": 12\n}\n\n# For surveys: spread across core themes\nsurvey_themes = [\"Summer School\", \"Mentoring\", \"Curriculum Modules\", \"Research Engagement\", \"Public Outreach\"]\nsi = {t: 0 for t in survey_themes}\nfor _ in range(counts_plan[\"survey\"]):\n    t = random.choice(survey_themes)\n    si[t] += 1\n    write_survey_appendix(si[t], t)\n\n# Transcripts: mostly Summer, Mentoring, Curriculum\ntranscript_themes = [\"Summer School\", \"Mentoring\", \"Curriculum Modules\"]\nti = {t: 0 for t in transcript_themes}\nfor _ in range(counts_plan[\"transcript\"]):\n    t = random.choice(transcript_themes)\n    ti[t] += 1\n    write_transcript_appendix(ti[t], t)\n\n# Observations: mix of Summer, Curriculum, Public Outreach\nobs_themes = [\"Summer School\", \"Curriculum Modules\", \"Public Outreach\"]\noi = {t: 0 for t in obs_themes}\nfor _ in range(counts_plan[\"observation\"]):\n    t = random.choice(obs_themes)\n    oi[t] += 1\n    write_observation_appendix(oi[t], t)\n\n# Curriculum module summaries\nfor i in range(1, counts_plan[\"curriculum\"] + 1):\n    write_curriculum_appendix(i)\n\n# Research engagement\nfor i in range(1, counts_plan[\"research\"] + 1):\n    write_research_engagement_appendix(i)\n\n# Collaboration analysis\nfor i in range(1, counts_plan[\"collaboration\"] + 1):\n    write_collaboration_appendix(i)\n\n# Ensure we have about 75 appendices\nlen_appendices = len(appendix_catalog)\n\n# Generate claims\nnum_claims = 600\nincorrect_ratio = 0.20\nnum_incorrect = int(num_claims * incorrect_ratio)\n\nclaim_rows = []\ngt_rows = []\n\ndef make_claim(app_meta, claim_id, correct: bool):\n    theme = app_meta[\"theme\"]\n    app_id = app_meta[\"app_id\"]\n    section_label = f\"Section 4.{random.randint(1,5)}\"\n    error_type = \"\"\n    claim_text = \"\"\n    detail = \"\"\n\n    if app_meta[\"type\"] == \"survey\":\n        ystat = random.choice(app_meta[\"stats\"])\n        y = ystat[\"year\"]\n        if correct:\n            claim_text = (\n                f\"In {y}, participants in the {theme.lower()} program reported an average \"\n                f\"satisfaction rating of approximately {ystat['mean']:.2f} on a 1â€“5 scale, \"\n                f\"with about {ystat['pct_high']:.1f}% selecting high values (4â€“5), as shown in Appendix {app_id}.\"\n            )\n        else:\n            wrong_mean = round(ystat[\"mean\"] + random.choice([-0.8, -0.5, 0.5, 0.8]), 2)\n            claim_text = (\n                f\"In {y}, satisfaction in the {theme.lower()} program dropped to around \"\n                f\"{wrong_mean:.2f} with fewer than 40% of participants choosing high values, \"\n                f\"according to Appendix {app_id}.\"\n            )\n            error_type = \"misreported_survey_stat\"\n        detail = f\"Table {app_id}.1\"\n\n    elif app_meta[\"type\"] == \"transcript\":\n        year = app_meta[\"years\"][0]\n        t_used = app_meta[\"themes\"]\n        all_possible = [\n            \"Interdisciplinarity\", \"Instructor Clarity\", \"Pacing\",\n            \"Labâ€“Computation Connection\", \"Mentoring Support\",\n            \"Collaboration Barriers\", \"Belonging\", \"Cognitive Overload\"\n        ]\n        if correct:\n            th = random.choice(t_used)\n            claim_text = (\n                f\"Focus group comments in Appendix {app_id} (from {year}) repeatedly highlight \"\n                f\"{th.lower()} as a salient theme in participants' experiences of the \"\n                f\"{theme.lower()} activities.\"\n            )\n        else:\n            th = random.choice([x for x in all_possible if x not in t_used] or all_possible)\n            claim_text = (\n                f\"Appendix {app_id} shows that {th.lower()} was the dominant recurring theme in \"\n                f\"participant discussions, with little attention paid to other issues.\"\n            )\n            error_type = \"qualitative_theme_mismatch\"\n        detail = \"Thematic codes section\"\n\n    elif app_meta[\"type\"] == \"observation\":\n        year = app_meta[\"years\"][0]\n        r = app_meta[\"rubric\"]\n        if correct:\n            claim_text = (\n                f\"Observation notes in Appendix {app_id} for {year} indicate relatively strong \"\n                f\"instructor clarity (rated {r['clarity']}/5) and high interactive engagement \"\n                f\"({r['engagement']}/5), with pacing rated at {r['pacing']}/5.\"\n            )\n        else:\n            claim_text = (\n                f\"Appendix {app_id} documents very low engagement scores (1/5) and uniformly \"\n                f\"poor clarity ratings for the observed {theme.lower()} session in {year}.\"\n            )\n            error_type = \"reversed_observation_ratings\"\n        detail = \"Rubric scores\"\n\n    elif app_meta[\"type\"] == \"curriculum\":\n        year = app_meta[\"years\"][0]\n        s = random.choice(app_meta[\"stats\"])\n        if correct:\n            claim_text = (\n                f\"For the {s['cohort']} cohort of the curriculum module documented in Appendix {app_id}, \"\n                f\"mean ratings were approximately {s['mean']:.2f} with about {s['pct_high']:.1f}% of students \"\n                f\"selecting high satisfaction (4â€“5).\"\n            )\n        else:\n            claim_text = (\n                f\"Appendix {app_id} shows that fewer than 30% of students in the {s['cohort']} cohort \"\n                f\"rated the module positively, with mean scores below 3.0.\"\n            )\n            error_type = \"fabricated_low_module_rating\"\n        detail = f\"Table {app_id}.1\"\n\n    elif app_meta[\"type\"] == \"research\":\n        s = random.choice(app_meta[\"stats\"])\n        if correct:\n            claim_text = (\n                f\"In {s['year']}, participants in the {theme.lower()} logs spent on average \"\n                f\"{s['mean_hrs']:.1f} hours on {s['activity'].lower()}, based on the synthetic data in Appendix {app_id}.\"\n            )\n        else:\n            claim_text = (\n                f\"Appendix {app_id} indicates that participants devoted less than 2 hours on average \"\n                f\"to {s['activity'].lower()} across all recorded years.\"\n            )\n            error_type = \"understated_research_hours\"\n        detail = f\"{s['year']} â€“ {s['activity']}\"\n\n    elif app_meta[\"type\"] == \"collaboration\":\n        net = app_meta[\"network\"]\n        year = app_meta[\"years\"][0]\n        if correct:\n            claim_text = (\n                f\"The {year} collaboration network in Appendix {app_id} includes {net['nodes']} participants \"\n                f\"and {net['edges']} edges, with an average degree of about {net['avg_degree']:.2f} and \"\n                f\"modularity near {net['modularity']:.2f}, indicating several semi-independent communities.\"\n            )\n        else:\n            claim_text = (\n                f\"Appendix {app_id} describes a very sparse {year} collaboration network with fewer than \"\n                f\"10 edges and no discernible community structure (modularity near 0.0).\"\n            )\n            error_type = \"mischaracterized_network_structure\"\n        detail = \"Network summary\"\n\n    claim_rows.append({\n        \"claim_id\": claim_id,\n        \"section_label\": section_label,\n        \"theme\": theme,\n        \"claim_text\": claim_text,\n        \"is_correct\": str(correct),\n        \"source_appendices\": app_id,\n        \"source_details\": detail,\n        \"error_type\": error_type,\n    })\n    gt_rows.append({\n        \"claim_id\": claim_id,\n        \"source_appendices\": app_id,\n        \"source_tables\": detail,\n        \"source_lines\": app_meta[\"type\"],\n        \"is_correct\": str(correct),\n        \"error_type\": error_type,\n    })\n\nfor i in range(num_claims):\n    cid = f\"C{i+1:03d}\"\n    correct = i >= num_incorrect  # first 20% incorrect\n    meta = random.choice(appendix_catalog)\n    make_claim(meta, cid, correct)\n\n# Write claims CSVs\nwith open(claims_dir / \"claims_catalog.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n    writer = csv.DictWriter(f, fieldnames=claim_rows[0].keys())\n    writer.writeheader()\n    writer.writerows(claim_rows)\n\nwith open(claims_dir / \"claims_ground_truth.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n    writer = csv.DictWriter(f, fieldnames=gt_rows[0].keys())\n    writer.writeheader()\n    writer.writerows(gt_rows)\n\n# Create a README description\nreadme = \"\"\"\nQCB-Style Synthetic Evaluation Corpus\n=====================================\n\nThis folder contains a synthetic, high-fidelity corpus designed to mimic a mixed-methods\nSTEM center evaluation environment (similar in spirit to QCB), without using any real data.\n\nStructure\n---------\n- appendices/: 75 text-based appendices that look like exported PDF content.\n  These include:\n  * Survey-style summaries with multi-year tables and satisfaction metrics.\n  * Focus group and interview transcripts with full conversational turns, coded themes,\n    and subcodes.\n  * Lecture and workshop observation notes with timestamps and rubric scores.\n  * Curriculum module summaries with learning outcomes and module ratings.\n  * Research engagement logs with hours by activity type and year.\n  * Collaboration network summaries with graph statistics (nodes, edges, density,\n    modularity, communities) and small adjacency matrix snippets.\n\n- claims/:\n  * claims_catalog.csv â€“ 600 synthetic claims that might appear in an evaluation\n    report. Around 20% are intentionally incorrect or misleading.\n  * claims_ground_truth.csv â€“ a private mapping file indicating, for each claim:\n    - whether it is correct,\n    - which appendix it should be traced to,\n    - what kind of error is present if incorrect.\n\nUsage\n-----\nThis corpus is intended for building and evaluating an \"Evidence Tracing & Explanation Agent\"\ncapstone project. An agent can:\n- Take a claim from claims_catalog.csv.\n- Retrieve and read the relevant appendix (and possibly others).\n- Decide whether the claim is supported by the evidence.\n- Produce a natural-language explanation pointing to the appropriate appendix and section.\n- Optionally, flag inconsistencies or missing evidence.\n\nAll numbers, participants, and narratives are fabricated, but internally consistent\nwithin each appendix. Any resemblance to real programs or data is coincidental.\n\"\"\"\n(base / \"README_SYNTHETIC_DATA.txt\").write_text(readme.strip() + \"\\n\")\n\n# Zip it\nzip_path = shutil.make_archive(\"qcb_capstone_data\", \"zip\", base_dir=base)\nzip_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:50.127013Z","iopub.execute_input":"2025-11-30T07:31:50.127271Z","iopub.status.idle":"2025-11-30T07:31:50.438844Z","shell.execute_reply.started":"2025-11-30T07:31:50.127252Z","shell.execute_reply":"2025-11-30T07:31:50.438094Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'qcb_capstone_data.zip'"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"### Project Structure","metadata":{}},{"cell_type":"code","source":"import os\n\nbase_dir = \"/kaggle/working/qcb_evidence_agent\"\ndirs = [\n    f\"{base_dir}/src\",\n    f\"{base_dir}/data/appendices\",\n    f\"{base_dir}/data/claims\"\n]\n\nfor d in dirs:\n    os.makedirs(d, exist_ok=True)\n    print(f\"Created: {d}\")\n\n# Create __init__.py so python can import from src\nwith open(f\"{base_dir}/src/__init__.py\", \"w\") as f:\n    f.write(\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:50.439718Z","iopub.execute_input":"2025-11-30T07:31:50.440100Z","iopub.status.idle":"2025-11-30T07:31:50.447540Z","shell.execute_reply.started":"2025-11-30T07:31:50.440072Z","shell.execute_reply":"2025-11-30T07:31:50.446236Z"}},"outputs":[{"name":"stdout","text":"Created: /kaggle/working/qcb_evidence_agent/src\nCreated: /kaggle/working/qcb_evidence_agent/data/appendices\nCreated: /kaggle/working/qcb_evidence_agent/data/claims\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Implementing the RAG Layer","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/qcb_evidence_agent/src/data_index.py\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nclass AppendixIndex:\n    def __init__(self, appendices_dir: str):\n        self.appendices_dir = Path(appendices_dir)\n        self.filepaths: List[Path] = []\n        self.texts: List[str] = []\n        self.appendix_ids: List[str] = []\n        self.vectorizer: TfidfVectorizer | None = None\n        self.tfidf_matrix = None\n\n    def load_appendices(self) -> None:\n        self.filepaths = sorted(self.appendices_dir.glob(\"*.txt\"))\n        self.texts = []\n        self.appendix_ids = []\n\n        for fp in self.filepaths:\n            text = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n            self.texts.append(text)\n            stem = fp.stem\n            parts = stem.split(\"_\")\n            app_id = parts[1] if len(parts) > 1 else stem\n            self.appendix_ids.append(app_id)\n\n    def build_index(self) -> None:\n        self.vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n        self.tfidf_matrix = self.vectorizer.fit_transform(self.texts)\n\n    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        if self.vectorizer is None:\n            raise RuntimeError(\"Index not built.\")\n\n        q_vec = self.vectorizer.transform([query])\n        scores = (self.tfidf_matrix @ q_vec.T).toarray().ravel()\n        top_idx = np.argsort(scores)[::-1][:top_k]\n\n        results = []\n        for idx in top_idx:\n            if scores[idx] <= 0: continue\n            full_text = self.texts[idx]\n            results.append({\n                \"appendix_id\": self.appendix_ids[idx],\n                \"filename\": str(self.filepaths[idx].name),\n                \"score\": float(scores[idx]),\n                \"snippet\": full_text[:800], # Increased snippet size for context\n                \"full_text\": full_text\n            })\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:50.449038Z","iopub.execute_input":"2025-11-30T07:31:50.449594Z","iopub.status.idle":"2025-11-30T07:31:50.465602Z","shell.execute_reply.started":"2025-11-30T07:31:50.449571Z","shell.execute_reply":"2025-11-30T07:31:50.464604Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/qcb_evidence_agent/src/data_index.py\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"%%writefile /kaggle/working/qcb_evidence_agent/src/tools.py\nfrom typing import List, Dict, Any\nfrom .data_index import AppendixIndex\n\nclass AppendixSearchTool:\n    def __init__(self, appendices_dir: str):\n        self.index = AppendixIndex(appendices_dir)\n        self.index.load_appendices()\n        self.index.build_index()\n\n    def search_appendices(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        return self.index.search(query_text, top_k=top_k)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:50.471300Z","iopub.execute_input":"2025-11-30T07:31:50.471793Z","iopub.status.idle":"2025-11-30T07:31:50.492213Z","shell.execute_reply.started":"2025-11-30T07:31:50.471755Z","shell.execute_reply":"2025-11-30T07:31:50.490871Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/qcb_evidence_agent/src/tools.py\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### Implement the Agents","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/qcb_evidence_agent/src/agents.py\nimport os\nimport json\nimport re\nimport time\nfrom typing import Any, Dict, List\nimport google.generativeai as genai\nfrom .tools import AppendixSearchTool\n\ndef get_gemini_model(model_name: str = \"gemini-2.0-flash\") -> genai.GenerativeModel:\n    api_key = os.environ.get(\"GEMINI_API_KEY\")\n    if not api_key:\n        print(\"ðŸš¨ CRITICAL WARNING: GEMINI_API_KEY is missing from environment variables!\")\n    genai.configure(api_key=api_key)\n    return genai.GenerativeModel(\n        model_name, \n        generation_config={\"response_mime_type\": \"application/json\"}\n    )\n\nclass ClaimUnderstandingAgent:\n    def __init__(self, model):\n        self.model = model\n\n    def understand_claim(self, claim_id: str, claim_text: str) -> Dict[str, Any]:\n        prompt = f'''Analyze this claim. Extract JSON. Claim: \"{claim_text}\"\n        Output: {{ \"claim_id\": \"{claim_id}\", \"claim_text\": \"{claim_text}\", \"key_concepts\": [list of strings] }}'''\n        \n        try:\n            # Retry logic for rate limits\n            try:\n                resp = self.model.generate_content(prompt)\n            except Exception as e:\n                if \"429\" in str(e):\n                    print(\"âš ï¸ Rate limit hit. Waiting 5 seconds...\")\n                    time.sleep(5)\n                    resp = self.model.generate_content(prompt)\n                else:\n                    raise e\n\n            parsed = json.loads(resp.text)\n            if isinstance(parsed, list): parsed = parsed[0]\n            return parsed\n        except Exception as e:\n            print(f\"âŒ ClaimUnderstandingAgent Error: {e}\")\n            return {\"claim_id\": claim_id, \"claim_text\": claim_text, \"key_concepts\": []}\n\nclass EvidenceRetrievalAgent:\n    def __init__(self, search_tool):\n        self.search_tool = search_tool\n\n    def retrieve_evidence(self, parsed_claim: Dict[str, Any]) -> Dict[str, Any]:\n        text = parsed_claim.get(\"claim_text\", \"\")\n        concepts = parsed_claim.get(\"key_concepts\", [])\n        if isinstance(concepts, list): concepts = \" \".join(str(c) for c in concepts)\n        query = f\"{text} {concepts}\"\n        \n        results = self.search_tool.search_appendices(query, top_k=4)\n        if not results:\n            print(f\"âš ï¸ Warning: No evidence found for query: '{query[:50]}...'\")\n            \n        return {\"parsed_claim\": parsed_claim, \"candidate_evidence\": results}\n\nclass EvidenceAnalysisAgent:\n    def __init__(self, model):\n        self.model = model\n\n    def analyze(self, retrieval_bundle: Dict[str, Any]) -> Dict[str, Any]:\n        claim = retrieval_bundle[\"parsed_claim\"]\n        evidence = retrieval_bundle[\"candidate_evidence\"]\n        \n        # Check if we actually have evidence\n        if not evidence:\n            return {\"verdict\": \"unsupported\", \"explanation\": \"No relevant evidence found in the appendices.\", \"evidence_appendix_ids\": []}\n\n        ev_text = \"\".join([f\"\\n[Appendix {e['appendix_id']}]: {e['snippet']}...\" for e in evidence])\n        \n        prompt = f'''Verify if claim is supported by evidence.\n        Claim: \"{claim.get('claim_text','')}\"\n        Evidence: {ev_text}\n        Output JSON: {{ \"verdict\": \"supported\"|\"unsupported\"|\"partially_supported\", \"evidence_appendix_ids\": [], \"explanation\": \"\" }}'''\n        \n        try:\n            try:\n                resp = self.model.generate_content(prompt)\n            except Exception as e:\n                if \"429\" in str(e):\n                    print(\"âš ï¸ Rate limit hit (Analysis). Waiting 5 seconds...\")\n                    time.sleep(5)\n                    resp = self.model.generate_content(prompt)\n                else:\n                    raise e\n                    \n            parsed = json.loads(resp.text)\n            if isinstance(parsed, list): parsed = parsed[0]\n            return parsed\n        except Exception as e:\n            print(f\"âŒ EvidenceAnalysisAgent Error: {e}\")\n            return {\"verdict\": \"error\", \"explanation\": f\"System Error: {str(e)}\", \"evidence_appendix_ids\": []}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:50.493227Z","iopub.execute_input":"2025-11-30T07:31:50.493513Z","iopub.status.idle":"2025-11-30T07:31:50.516715Z","shell.execute_reply.started":"2025-11-30T07:31:50.493490Z","shell.execute_reply":"2025-11-30T07:31:50.515641Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/qcb_evidence_agent/src/agents.py\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### Orchestrator","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/qcb_evidence_agent/src/orchestrator.py\nimport pandas as pd\nfrom .agents import get_gemini_model, ClaimUnderstandingAgent, EvidenceRetrievalAgent, EvidenceAnalysisAgent\nfrom .tools import AppendixSearchTool\n\nclass EvidenceTracingOrchestrator:\n    def __init__(self, data_dir: str):\n        self.data_dir = data_dir\n        self.model = get_gemini_model()\n        \n        # Initialize Tools\n        self.search_tool = AppendixSearchTool(f\"{data_dir}/data/appendices\")\n        \n        # Initialize Agents\n        self.understander = ClaimUnderstandingAgent(self.model)\n        self.retriever = EvidenceRetrievalAgent(self.search_tool)\n        self.analyzer = EvidenceAnalysisAgent(self.model)\n        \n        # Load claims\n        self.claims_df = pd.read_csv(f\"{data_dir}/data/claims/claims_catalog.csv\")\n\n    def review_claim(self, claim_id: str):\n        # 0. Get Data\n        row = self.claims_df[self.claims_df[\"claim_id\"] == claim_id].iloc[0]\n        \n        # 1. Understand\n        parsed = self.understander.understand_claim(claim_id, row[\"claim_text\"])\n        \n        # 2. Retrieve\n        context = self.retriever.retrieve_evidence(parsed)\n        \n        # 3. Analyze\n        result = self.analyzer.analyze(context)\n        \n        return {\n            \"claim_id\": claim_id,\n            \"original_text\": row[\"claim_text\"],\n            \"verdict\": result[\"verdict\"],\n            \"explanation\": result[\"explanation\"],\n            \"cited_appendices\": result[\"evidence_appendix_ids\"]\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:50.517746Z","iopub.execute_input":"2025-11-30T07:31:50.518106Z","iopub.status.idle":"2025-11-30T07:31:50.541135Z","shell.execute_reply.started":"2025-11-30T07:31:50.518077Z","shell.execute_reply":"2025-11-30T07:31:50.539918Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/qcb_evidence_agent/src/orchestrator.py\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Final Execution","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\n# Try to load from Kaggle Secrets first\ntry:\n    user_secrets = UserSecretsClient()\n    # Ensure you added the secret named 'GEMINI_API_KEY' in the Add-ons menu!\n    api_key = user_secrets.get_secret(\"GEMINI_API_KEY\") \n    os.environ[\"GEMINI_API_KEY\"] = api_key\n    print(\"âœ… API Key successfully loaded from Kaggle Secrets.\")\nexcept Exception as e:\n    # Fallback: If secrets aren't set up, paste it manually\n    print(\"âš ï¸ Could not find Kaggle Secret 'GEMINI_API_KEY'.\")\n    print(\"Please paste your API Key below:\")\n    os.environ[\"GEMINI_API_KEY\"] = input(\"Enter your Gemini API Key: \").strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:50.542429Z","iopub.execute_input":"2025-11-30T07:31:50.542967Z","iopub.status.idle":"2025-11-30T07:31:50.672523Z","shell.execute_reply.started":"2025-11-30T07:31:50.542930Z","shell.execute_reply":"2025-11-30T07:31:50.670958Z"}},"outputs":[{"name":"stdout","text":"âœ… API Key successfully loaded from Kaggle Secrets.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import sys\n# Add our source code to python path\nsys.path.append(\"/kaggle/working/qcb_evidence_agent\")\n\nfrom src.orchestrator import EvidenceTracingOrchestrator\n\n# Initialize the system\nprint(\"Initializing Orchestrator (Building Index)...\")\norchestrator = EvidenceTracingOrchestrator(\"/kaggle/working/qcb_evidence_agent\")\n\n# Pick a claim to test\ntarget_claim = \"C075\"\n\nprint(f\"\\n--- Processing Claim {target_claim} ---\\n\")\nresult = orchestrator.review_claim(target_claim)\n\n# Display Results\nprint(f\"Claim: {result['original_text']}\")\nprint(f\"Verdict: {result['verdict'].upper()}\")\nprint(f\"Explanation: {result['explanation']}\")\nprint(f\"Sources: {result['cited_appendices']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:50.674070Z","iopub.execute_input":"2025-11-30T07:31:50.674394Z","iopub.status.idle":"2025-11-30T07:31:52.817762Z","shell.execute_reply.started":"2025-11-30T07:31:50.674365Z","shell.execute_reply":"2025-11-30T07:31:52.816711Z"}},"outputs":[{"name":"stdout","text":"Initializing Orchestrator (Building Index)...\n\n--- Processing Claim C075 ---\n\nClaim: In 2023, satisfaction in the research engagement program dropped to around 3.95 with fewer than 40% of participants choosing high values, according to Appendix R3.\nVerdict: UNSUPPORTED\nExplanation: Appendix R3 shows the mean satisfaction in 2023 was 4.45 and 61.0% of participants chose high values (4-5). This contradicts the claim that satisfaction dropped to 3.95 and fewer than 40% chose high values.\nSources: ['R3']\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"### Evaluation ","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/qcb_evidence_agent/src/evaluation.py\nimport pandas as pd\nimport time\nfrom typing import Dict, Any, List\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom .orchestrator import EvidenceTracingOrchestrator\n\ndef normalize_verdict(v: str) -> str:\n    v = str(v).lower().strip()\n    if v in [\"supported\", \"true\", \"yes\"]:\n        return \"supported\"\n    if v in [\"unsupported\", \"false\", \"no\"]:\n        return \"unsupported\"\n    return \"partially_supported\"\n\ndef normalize_gt(gt: str) -> str:\n    return \"supported\" if str(gt) == \"True\" else \"unsupported\"\n\ndef run_batch_eval(data_dir: str, max_claims: int = 20) -> pd.DataFrame:\n    print(f\"Starting evaluation on {max_claims} claims...\")\n    \n    orch = EvidenceTracingOrchestrator(data_dir)\n    \n    claims_path = f\"{data_dir}/data/claims/claims_catalog.csv\"\n    gt_path = f\"{data_dir}/data/claims/claims_ground_truth.csv\"\n    \n    df_claims = pd.read_csv(claims_path)\n    df_gt = pd.read_csv(gt_path)\n    \n    # 1. Prepare Ground Truth\n    gt_subset = df_gt[['claim_id', 'is_correct']].copy()\n    gt_subset = gt_subset.rename(columns={'is_correct': 'gt_is_correct'})\n    \n    # 2. Merge\n    df_merged = pd.merge(df_claims, gt_subset, on='claim_id', how='inner')\n    \n    # 3. Sample\n    df_sample = df_merged.sample(n=min(max_claims, len(df_merged)), random_state=42)\n    \n    results = []\n    \n    for idx, row in df_sample.iterrows():\n        cid = row['claim_id']\n        text = row['claim_text']\n        gt_raw = row['gt_is_correct']\n        \n        try:\n            # Run Agent\n            prediction = orch.review_claim(cid)\n            pred_verdict = prediction.get('verdict', 'error')\n            \n            y_pred = normalize_verdict(pred_verdict)\n            y_true = normalize_gt(gt_raw)\n            \n            results.append({\n                \"claim_id\": cid,\n                \"claim_text\": text,\n                \"ground_truth_raw\": gt_raw,\n                \"agent_verdict_raw\": pred_verdict,\n                \"y_true\": y_true,\n                \"y_pred\": y_pred,\n                \"explanation\": prediction.get('explanation', '')\n            })\n            print(f\"Processed {cid}: GT={y_true} | Pred={y_pred}\")\n            \n            # --- FIX: SLEEP TO RESPECT RATE LIMITS ---\n            # 15 requests/min = 1 request every 4 seconds. \n            # We do ~2 requests per claim, so we need to wait ~8 seconds.\n            time.sleep(10) \n            \n        except Exception as e:\n            print(f\"Error processing {cid}: {e}\")\n            time.sleep(10) # Wait even on error to let quota reset\n            \n    return pd.DataFrame(results)\n\ndef print_report(df_results: pd.DataFrame):\n    print(\"\\n\" + \"=\"*40)\n    print(\"EVALUATION REPORT\")\n    print(\"=\"*40)\n    \n    if df_results.empty:\n        print(\"No results to show.\")\n        return\n\n    y_true = df_results['y_true']\n    y_pred = df_results['y_pred']\n    \n    valid_mask = y_pred != 'error'\n    \n    if valid_mask.sum() == 0:\n        print(\"No valid predictions found.\")\n        return\n\n    acc = accuracy_score(y_true[valid_mask], y_pred[valid_mask])\n    print(f\"Accuracy: {acc:.2%}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true[valid_mask], y_pred[valid_mask], zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:52.818713Z","iopub.execute_input":"2025-11-30T07:31:52.819053Z","iopub.status.idle":"2025-11-30T07:31:52.827218Z","shell.execute_reply.started":"2025-11-30T07:31:52.819030Z","shell.execute_reply":"2025-11-30T07:31:52.825997Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/qcb_evidence_agent/src/evaluation.py\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import sys\nimport importlib\nsys.path.append(\"/kaggle/working/qcb_evidence_agent\")\n\n# Force Reload\nimport src.evaluation\nimportlib.reload(src.evaluation)\n\nfrom src.evaluation import run_batch_eval, print_report\n\n# Run again\ndf_results = run_batch_eval(\"/kaggle/working/qcb_evidence_agent\", max_claims=20)\nprint_report(df_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:31:52.828167Z","iopub.execute_input":"2025-11-30T07:31:52.828457Z","iopub.status.idle":"2025-11-30T07:35:53.302322Z","shell.execute_reply.started":"2025-11-30T07:31:52.828435Z","shell.execute_reply":"2025-11-30T07:35:53.301161Z"}},"outputs":[{"name":"stdout","text":"Starting evaluation on 20 claims...\nProcessed C111: GT=unsupported | Pred=unsupported\nProcessed C420: GT=supported | Pred=supported\nProcessed C566: GT=supported | Pred=supported\nProcessed C078: GT=unsupported | Pred=unsupported\nProcessed C182: GT=supported | Pred=supported\nProcessed C285: GT=supported | Pred=supported\nProcessed C011: GT=unsupported | Pred=unsupported\nProcessed C470: GT=supported | Pred=supported\nProcessed C079: GT=unsupported | Pred=unsupported\nProcessed C350: GT=supported | Pred=supported\nProcessed C056: GT=unsupported | Pred=unsupported\nProcessed C119: GT=unsupported | Pred=unsupported\nProcessed C110: GT=unsupported | Pred=unsupported\nProcessed C589: GT=supported | Pred=supported\nProcessed C370: GT=supported | Pred=supported\nProcessed C235: GT=supported | Pred=unsupported\nProcessed C031: GT=unsupported | Pred=unsupported\nProcessed C213: GT=supported | Pred=supported\nProcessed C185: GT=supported | Pred=supported\nProcessed C087: GT=unsupported | Pred=unsupported\n\n========================================\nEVALUATION REPORT\n========================================\nAccuracy: 95.00%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n   supported       1.00      0.91      0.95        11\n unsupported       0.90      1.00      0.95         9\n\n    accuracy                           0.95        20\n   macro avg       0.95      0.95      0.95        20\nweighted avg       0.96      0.95      0.95        20\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"### Refining?","metadata":{}},{"cell_type":"markdown","source":"### Personalized Claims Chatbot UI","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/qcb_evidence_agent/src/orchestrator_2.py\nfrom typing import Dict, Any\nfrom .agents import get_gemini_model, ClaimUnderstandingAgent, EvidenceRetrievalAgent, EvidenceAnalysisAgent\nfrom .tools import AppendixSearchTool\n\nclass ChatOrchestrator:\n    def __init__(self, data_dir: str):\n        self.data_dir = data_dir\n        self.model = get_gemini_model()\n        \n        # Initialize Tools (Shared with the other orchestrator)\n        self.search_tool = AppendixSearchTool(f\"{data_dir}/data/appendices\")\n        \n        # Initialize Agents\n        self.understander = ClaimUnderstandingAgent(self.model)\n        self.retriever = EvidenceRetrievalAgent(self.search_tool)\n        self.analyzer = EvidenceAnalysisAgent(self.model)\n\n    def review_freeform_claim(self, claim_text: str) -> Dict[str, Any]:\n        \"\"\"\n        Takes raw user input, runs the agent pipeline, and returns the result.\n        No CSV lookup required.\n        \"\"\"\n        # 1. Understand (Use 'user_query' as a dummy ID)\n        parsed = self.understander.understand_claim(\"user_query\", claim_text)\n        \n        # 2. Retrieve\n        context = self.retriever.retrieve_evidence(parsed)\n        \n        # 3. Analyze\n        result = self.analyzer.analyze(context)\n        \n        # 4. Return structured response for the UI\n        return {\n            \"original_text\": claim_text,\n            \"verdict\": result.get(\"verdict\", \"error\"),\n            \"explanation\": result.get(\"explanation\", \"No explanation provided.\"),\n            \"cited_appendices\": result.get(\"evidence_appendix_ids\", []),\n            \"evidence_snippets\": context.get(\"candidate_evidence\", []) \n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:35:53.303477Z","iopub.execute_input":"2025-11-30T07:35:53.303759Z","iopub.status.idle":"2025-11-30T07:35:53.311111Z","shell.execute_reply.started":"2025-11-30T07:35:53.303737Z","shell.execute_reply":"2025-11-30T07:35:53.310122Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/qcb_evidence_agent/src/orchestrator_2.py\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"%%writefile /kaggle/working/qcb_evidence_agent/app.py\nimport streamlit as st\nimport os\nimport sys\n\n# Ensure src is in pythonpath\nsys.path.append(os.getcwd())\n\n# Import the new Chat Orchestrator\nfrom src.orchestrator_2 import ChatOrchestrator\n\n# Config\nDATA_DIR = \"./qcb_evidence_agent\" \nst.set_page_config(page_title=\"Evidence Agent Chat\", layout=\"wide\")\n\n@st.cache_resource\ndef get_agent():\n    # Load the chat-specific agent\n    return ChatOrchestrator(DATA_DIR)\n\nst.title(\"ðŸ•µï¸â€â™€ï¸ Evidence Tracing Chatbot\")\nst.markdown(\"Type any claim below, and I will check the synthetic evidence base for you.\")\n\n# Input Area\nclaim_text = st.text_area(\"Enter your claim:\", height=100, placeholder=\"e.g., Student confidence in coding tasks improved significantly in 2024.\")\n\nif st.button(\"Verify Claim\"):\n    if not claim_text:\n        st.warning(\"Please enter some text first.\")\n    else:\n        agent = get_agent()\n        \n        with st.spinner(\"Analyzing evidence...\"):\n            # Call the freeform method\n            result = agent.review_freeform_claim(claim_text)\n        \n        # 1. Show Verdict\n        verdict = result['verdict'].upper()\n        \n        # Dynamic color coding\n        if \"SUPPORTED\" in verdict and \"PARTIALLY\" not in verdict:\n            color = \"green\"\n        elif \"UNSUPPORTED\" in verdict:\n            color = \"red\"\n        else:\n            color = \"orange\" # Partial or Error\n            \n        st.markdown(f\"### Verdict: :{color}[{verdict}]\")\n        \n        # 2. Show Explanation\n        st.markdown(f\"**Analysis:** {result['explanation']}\")\n        \n        # 3. Show Cited Evidence\n        if result['evidence_snippets']:\n            st.divider()\n            st.subheader(\"ðŸ“š Evidence Retrieved\")\n            for ev in result['evidence_snippets']:\n                with st.expander(f\"Appendix {ev['appendix_id']} (Relevance: {ev['score']:.2f})\"):\n                    st.info(f\"Source File: {ev['filename']}\")\n                    st.text(ev['snippet'])\n        else:\n            st.info(\"No relevant evidence found in the appendices.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:35:53.312186Z","iopub.execute_input":"2025-11-30T07:35:53.312534Z","iopub.status.idle":"2025-11-30T07:35:53.336380Z","shell.execute_reply.started":"2025-11-30T07:35:53.312504Z","shell.execute_reply":"2025-11-30T07:35:53.335203Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/qcb_evidence_agent/app.py\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\n\n# Ensure key is loaded\nif \"GEMINI_API_KEY\" not in os.environ:\n    from kaggle_secrets import UserSecretsClient\n    os.environ[\"GEMINI_API_KEY\"] = UserSecretsClient().get_secret(\"GEMINI_API_KEY\")\n\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n\nprint(\"Checking available models...\")\ntry:\n    for m in genai.list_models():\n        if 'generateContent' in m.supported_generation_methods:\n            print(f\"- {m.name}\")\nexcept Exception as e:\n    print(f\"Error listing models: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:35:53.337415Z","iopub.execute_input":"2025-11-30T07:35:53.337807Z","iopub.status.idle":"2025-11-30T07:35:53.689560Z","shell.execute_reply.started":"2025-11-30T07:35:53.337782Z","shell.execute_reply":"2025-11-30T07:35:53.688396Z"}},"outputs":[{"name":"stdout","text":"Checking available models...\n- models/gemini-2.5-pro-preview-03-25\n- models/gemini-2.5-flash\n- models/gemini-2.5-pro-preview-05-06\n- models/gemini-2.5-pro-preview-06-05\n- models/gemini-2.5-pro\n- models/gemini-2.0-flash-exp\n- models/gemini-2.0-flash\n- models/gemini-2.0-flash-001\n- models/gemini-2.0-flash-exp-image-generation\n- models/gemini-2.0-flash-lite-001\n- models/gemini-2.0-flash-lite\n- models/gemini-2.0-flash-lite-preview-02-05\n- models/gemini-2.0-flash-lite-preview\n- models/gemini-2.0-pro-exp\n- models/gemini-2.0-pro-exp-02-05\n- models/gemini-exp-1206\n- models/gemini-2.0-flash-thinking-exp-01-21\n- models/gemini-2.0-flash-thinking-exp\n- models/gemini-2.0-flash-thinking-exp-1219\n- models/gemini-2.5-flash-preview-tts\n- models/gemini-2.5-pro-preview-tts\n- models/learnlm-2.0-flash-experimental\n- models/gemma-3-1b-it\n- models/gemma-3-4b-it\n- models/gemma-3-12b-it\n- models/gemma-3-27b-it\n- models/gemma-3n-e4b-it\n- models/gemma-3n-e2b-it\n- models/gemini-flash-latest\n- models/gemini-flash-lite-latest\n- models/gemini-pro-latest\n- models/gemini-2.5-flash-lite\n- models/gemini-2.5-flash-image-preview\n- models/gemini-2.5-flash-image\n- models/gemini-2.5-flash-preview-09-2025\n- models/gemini-2.5-flash-lite-preview-09-2025\n- models/gemini-3-pro-preview\n- models/gemini-3-pro-image-preview\n- models/nano-banana-pro-preview\n- models/gemini-robotics-er-1.5-preview\n- models/gemini-2.5-computer-use-preview-10-2025\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"### Text Only Checking (Initial Pass)\nimport sys\nsys.path.append(\"/kaggle/working/qcb_evidence_agent\")\n\nfrom src.orchestrator_2 import ChatOrchestrator\n\nprint(\"Initializing Chat Agent...\")\nchat_agent = ChatOrchestrator(\"/kaggle/working/qcb_evidence_agent\")\n\n# Test a custom query\nuser_query = \"In the 2023 mechanistic modeling session, the instructor frequently checked in with students and explicitly connected the computation to the experimental context, demonstrating consistently high instructor clarity\"\nprint(f\"\\nUser Query: {user_query}\")\nprint(\"-\" * 40)\nresult = chat_agent.review_freeform_claim(user_query)\n\nprint(f\"VERDICT: {result['verdict'].upper()}\")\nprint(f\"REASONING: {result['explanation']}\")\n\nuser_query_2 = \"In the 2023 summer school, participants consistently described a strong sense of improved belonging throughout the program\"\nprint(f\"\\nUser Query: {user_query_2}\")\nprint(\"-\" * 40)\nresult_2 = chat_agent.review_freeform_claim(user_query_2)\n\nprint(f\"VERDICT: {result_2['verdict'].upper()}\")\nprint(f\"REASONING: {result_2['explanation']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:35:53.690525Z","iopub.execute_input":"2025-11-30T07:35:53.690855Z","iopub.status.idle":"2025-11-30T07:35:57.552723Z","shell.execute_reply.started":"2025-11-30T07:35:53.690828Z","shell.execute_reply":"2025-11-30T07:35:57.551681Z"}},"outputs":[{"name":"stdout","text":"Initializing Chat Agent...\n\nUser Query: In the 2023 mechanistic modeling session, the instructor frequently checked in with students and explicitly connected the computation to the experimental context, demonstrating consistently high instructor clarity\n----------------------------------------\nVERDICT: SUPPORTED\nREASONING: The claim states that the instructor frequently checked in with students and explicitly connected the computation to the experimental context. The provided evidence from Appendices A1, A3, A5, and P2 all contain observations noting that the instructor checked in with groups, and participants asked for clarification, and that the instructor explicitly connected the computation to the experimental context. This supports the claim.\n\nUser Query: In the 2023 summer school, participants consistently described a strong sense of improved belonging throughout the program\n----------------------------------------\nVERDICT: UNSUPPORTED\nREASONING: The provided transcripts do not contain statements about improved belonging in the 2023 summer school. Participants in Appendix A6 from the 2024 summer school mentioned belonging, but the claim is specific to the 2023 summer school.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport zipfile\n\n# 1. Create the project skeleton\nbase_dir = \"/kaggle/working/qcb_evidence_agent\"\nsrc_dir = f\"{base_dir}/src\"\ndata_dir = f\"{base_dir}/data\"\n\nos.makedirs(src_dir, exist_ok=True)\nos.makedirs(data_dir, exist_ok=True)\n\n# 2. Unzip your data (Assuming your zip is named 'qcb_capstone_data.zip' and in /kaggle/working/)\n# If you uploaded it via the \"Add Input\" menu, it might be in /kaggle/input/\nzip_path = \"/kaggle/working/qcb_capstone_data.zip\" \n\nif os.path.exists(zip_path):\n    print(\"Unzipping data...\")\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(data_dir)\n    print(\"Data unzipped successfully.\")\nelse:\n    print(f\"Warning: Could not find {zip_path}. Please ensure the file is uploaded.\")\n    # Check if we need to move files if the zip created a subfolder\n    # (Adjust this logic if your zip structure is different)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:35:57.553876Z","iopub.execute_input":"2025-11-30T07:35:57.554256Z","iopub.status.idle":"2025-11-30T07:35:57.602548Z","shell.execute_reply.started":"2025-11-30T07:35:57.554235Z","shell.execute_reply":"2025-11-30T07:35:57.601416Z"}},"outputs":[{"name":"stdout","text":"Unzipping data...\nData unzipped successfully.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import os\n\n# --- 1. Create __init__.py ---\nwith open(\"/kaggle/working/qcb_evidence_agent/src/__init__.py\", \"w\") as f:\n    f.write(\"\")\n\n# --- 2. Create data_index.py (Search Engine) ---\ndata_index_code = \"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nclass AppendixIndex:\n    def __init__(self, appendices_dir: str):\n        self.appendices_dir = Path(appendices_dir)\n        self.filepaths: List[Path] = []\n        self.texts: List[str] = []\n        self.appendix_ids: List[str] = []\n        self.vectorizer: TfidfVectorizer | None = None\n        self.tfidf_matrix = None\n\n    def load_appendices(self) -> None:\n        # Search recursively in case of nested folders\n        self.filepaths = sorted(list(self.appendices_dir.glob(\"**/*.txt\")))\n        self.texts = []\n        self.appendix_ids = []\n\n        for fp in self.filepaths:\n            text = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n            self.texts.append(text)\n            stem = fp.stem\n            parts = stem.split(\"_\")\n            app_id = parts[1] if len(parts) > 1 else stem\n            self.appendix_ids.append(app_id)\n            \n        if not self.texts:\n            print(f\"Warning: No .txt files found in {self.appendices_dir}\")\n\n    def build_index(self) -> None:\n        if not self.texts:\n             self.vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n             self.tfidf_matrix = self.vectorizer.fit_transform([\"dummy\"])\n             return\n\n        self.vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n        self.tfidf_matrix = self.vectorizer.fit_transform(self.texts)\n\n    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        if self.vectorizer is None:\n            raise RuntimeError(\"Index not built.\")\n\n        q_vec = self.vectorizer.transform([query])\n        scores = (self.tfidf_matrix @ q_vec.T).toarray().ravel()\n        top_idx = np.argsort(scores)[::-1][:top_k]\n\n        results = []\n        for idx in top_idx:\n            if scores[idx] <= 0 or idx >= len(self.texts): continue\n            \n            full_text = self.texts[idx]\n            results.append({\n                \"appendix_id\": self.appendix_ids[idx],\n                \"filename\": str(self.filepaths[idx].name),\n                \"score\": float(scores[idx]),\n                \"snippet\": full_text[:800], \n                \"full_text\": full_text\n            })\n        return results\n\"\"\"\nwith open(\"/kaggle/working/qcb_evidence_agent/src/data_index.py\", \"w\") as f:\n    f.write(data_index_code)\n\n# --- 3. Create tools.py ---\ntools_code = \"\"\"\nfrom typing import List, Dict, Any\nfrom .data_index import AppendixIndex\n\nclass AppendixSearchTool:\n    def __init__(self, appendices_dir: str):\n        self.index = AppendixIndex(appendices_dir)\n        self.index.load_appendices()\n        self.index.build_index()\n\n    def search_appendices(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        return self.index.search(query_text, top_k=top_k)\n\"\"\"\nwith open(\"/kaggle/working/qcb_evidence_agent/src/tools.py\", \"w\") as f:\n    f.write(tools_code)\n\n# --- 4. Create agents.py (Robust Version) ---\nagents_code = \"\"\"\nimport os\nimport json\nimport re\nfrom typing import Any, Dict, List\nimport google.generativeai as genai\nfrom .tools import AppendixSearchTool\n\ndef get_gemini_model(model_name: str = \"gemini-2.0-flash\") -> genai.GenerativeModel:\n    api_key = os.environ.get(\"GEMINI_API_KEY\")\n    if not api_key:\n        print(\"Warning: GEMINI_API_KEY not found.\")\n    genai.configure(api_key=api_key)\n    return genai.GenerativeModel(model_name, generation_config={\"response_mime_type\": \"application/json\"})\n\nclass ClaimUnderstandingAgent:\n    def __init__(self, model):\n        self.model = model\n\n    def understand_claim(self, claim_id: str, claim_text: str) -> Dict[str, Any]:\n        prompt = f'''Analyze this claim. Extract JSON. Claim: \"{claim_text}\"\n        Output: {{ \"claim_id\": \"{claim_id}\", \"claim_text\": \"{claim_text}\", \"key_concepts\": [list of strings] }}'''\n        \n        try:\n            resp = self.model.generate_content(prompt)\n            parsed = json.loads(resp.text)\n            if isinstance(parsed, list): parsed = parsed[0]\n            return parsed\n        except:\n            return {\"claim_id\": claim_id, \"claim_text\": claim_text, \"key_concepts\": []}\n\nclass EvidenceRetrievalAgent:\n    def __init__(self, search_tool):\n        self.search_tool = search_tool\n\n    def retrieve_evidence(self, parsed_claim: Dict[str, Any]) -> Dict[str, Any]:\n        text = parsed_claim.get(\"claim_text\", \"\")\n        concepts = parsed_claim.get(\"key_concepts\", [])\n        if isinstance(concepts, list): concepts = \" \".join(str(c) for c in concepts)\n        query = f\"{text} {concepts}\"\n        return {\"parsed_claim\": parsed_claim, \"candidate_evidence\": self.search_tool.search_appendices(query, top_k=4)}\n\nclass EvidenceAnalysisAgent:\n    def __init__(self, model):\n        self.model = model\n\n    def analyze(self, retrieval_bundle: Dict[str, Any]) -> Dict[str, Any]:\n        claim = retrieval_bundle[\"parsed_claim\"]\n        evidence = retrieval_bundle[\"candidate_evidence\"]\n        ev_text = \"\".join([f\"\\\\n[Appendix {e['appendix_id']}]: {e['snippet']}...\" for e in evidence])\n        \n        prompt = f'''Verify if claim is supported by evidence.\n        Claim: \"{claim.get('claim_text','')}\"\n        Evidence: {ev_text}\n        Output JSON: {{ \"verdict\": \"supported\"|\"unsupported\"|\"partially_supported\", \"evidence_appendix_ids\": [], \"explanation\": \"\" }}'''\n        \n        try:\n            resp = self.model.generate_content(prompt)\n            parsed = json.loads(resp.text)\n            if isinstance(parsed, list): parsed = parsed[0]\n            return parsed\n        except:\n            return {\"verdict\": \"error\", \"explanation\": \"Analysis failed\", \"evidence_appendix_ids\": []}\n\"\"\"\nwith open(\"/kaggle/working/qcb_evidence_agent/src/agents.py\", \"w\") as f:\n    f.write(agents_code)\n\n# --- 5. Create orchestrator_2.py (Chat Logic) ---\norch_code = \"\"\"\nfrom typing import Dict, Any\nfrom .agents import get_gemini_model, ClaimUnderstandingAgent, EvidenceRetrievalAgent, EvidenceAnalysisAgent\nfrom .tools import AppendixSearchTool\n\nclass ChatOrchestrator:\n    def __init__(self, data_dir: str):\n        self.data_dir = data_dir\n        self.model = get_gemini_model()\n        self.search_tool = AppendixSearchTool(f\"{data_dir}/data/appendices\")\n        self.understander = ClaimUnderstandingAgent(self.model)\n        self.retriever = EvidenceRetrievalAgent(self.search_tool)\n        self.analyzer = EvidenceAnalysisAgent(self.model)\n\n    def review_freeform_claim(self, claim_text: str) -> Dict[str, Any]:\n        parsed = self.understander.understand_claim(\"user_query\", claim_text)\n        context = self.retriever.retrieve_evidence(parsed)\n        result = self.analyzer.analyze(context)\n        return {\n            \"original_text\": claim_text,\n            \"verdict\": result.get(\"verdict\", \"error\"),\n            \"explanation\": result.get(\"explanation\", \"No explanation provided.\"),\n            \"evidence_snippets\": context.get(\"candidate_evidence\", [])\n        }\n\"\"\"\nwith open(\"/kaggle/working/qcb_evidence_agent/src/orchestrator_2.py\", \"w\") as f:\n    f.write(orch_code)\n\n# --- 6. Create app.py (Streamlit UI) ---\napp_code = \"\"\"\nimport streamlit as st\nimport os, sys\nsys.path.append(os.getcwd())\nfrom src.orchestrator_2 import ChatOrchestrator\n\nDATA_DIR = \"./qcb_evidence_agent\" \nst.set_page_config(page_title=\"Evidence Chat\", layout=\"wide\")\n\n@st.cache_resource\ndef get_agent():\n    return ChatOrchestrator(DATA_DIR)\n\nst.title(\"ðŸ•µï¸â€â™€ï¸ Evidence Tracing Chatbot\")\nclaim = st.text_area(\"Enter claim:\")\nif st.button(\"Verify\") and claim:\n    with st.spinner(\"Analyzing...\"):\n        res = get_agent().review_freeform_claim(claim)\n    st.markdown(f\"### Verdict: {res['verdict'].upper()}\")\n    st.write(res['explanation'])\n    if res['evidence_snippets']:\n        st.divider()\n        for e in res['evidence_snippets']:\n            with st.expander(f\"Appendix {e['appendix_id']}\"):\n                st.text(e['snippet'])\n\"\"\"\nwith open(\"/kaggle/working/qcb_evidence_agent/app.py\", \"w\") as f:\n    f.write(app_code)\n\nprint(\"âœ… All system files have been restored successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:35:57.603716Z","iopub.execute_input":"2025-11-30T07:35:57.604059Z","iopub.status.idle":"2025-11-30T07:35:57.619576Z","shell.execute_reply.started":"2025-11-30T07:35:57.604030Z","shell.execute_reply":"2025-11-30T07:35:57.618217Z"}},"outputs":[{"name":"stdout","text":"âœ… All system files have been restored successfully!\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# Zip the entire project folder\nshutil.make_archive(\"/kaggle/working/evidence_agent_full_project\", 'zip', \"/kaggle/working/qcb_evidence_agent\")\n\nprint(\"Click below to download your full project:\")\ndisplay(FileLink('evidence_agent_full_project.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:35:57.709587Z","iopub.execute_input":"2025-11-30T07:35:57.710027Z","iopub.status.idle":"2025-11-30T07:35:57.786713Z","shell.execute_reply.started":"2025-11-30T07:35:57.709979Z","shell.execute_reply":"2025-11-30T07:35:57.785568Z"}},"outputs":[{"name":"stdout","text":"Click below to download your full project:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/evidence_agent_full_project.zip","text/html":"<a href='evidence_agent_full_project.zip' target='_blank'>evidence_agent_full_project.zip</a><br>"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FINAL SHOWDOWN","metadata":{}},{"cell_type":"code","source":"import os\n\n# 1. Create Directories\nos.makedirs(\"/kaggle/working/qcb_evidence_agent/src\", exist_ok=True)\nos.makedirs(\"/kaggle/working/qcb_evidence_agent/data\", exist_ok=True)\n\n# 2. Create __init__.py\nwith open(\"/kaggle/working/qcb_evidence_agent/src/__init__.py\", \"w\") as f:\n    f.write(\"\")\n\n# 3. Create agents.py (With Error Printing Logic)\nagents_code = \"\"\"\nimport os\nimport json\nimport re\nimport time\nfrom typing import Any, Dict, List\nimport google.generativeai as genai\nfrom .tools import AppendixSearchTool\n\ndef get_gemini_model(model_name: str = \"gemini-2.0-flash\") -> genai.GenerativeModel:\n    api_key = os.environ.get(\"GEMINI_API_KEY\")\n    if not api_key:\n        print(\"ðŸš¨ CRITICAL WARNING: GEMINI_API_KEY is missing!\")\n    genai.configure(api_key=api_key)\n    return genai.GenerativeModel(model_name, generation_config={\"response_mime_type\": \"application/json\"})\n\nclass ClaimUnderstandingAgent:\n    def __init__(self, model):\n        self.model = model\n\n    def understand_claim(self, claim_id: str, claim_text: str) -> Dict[str, Any]:\n        prompt = f'''Analyze this claim. Extract JSON. Claim: \"{claim_text}\"\n        Output: {{ \"claim_id\": \"{claim_id}\", \"claim_text\": \"{claim_text}\", \"key_concepts\": [list of strings] }}'''\n        try:\n            resp = self.model.generate_content(prompt)\n            parsed = json.loads(resp.text)\n            if isinstance(parsed, list): parsed = parsed[0]\n            return parsed\n        except Exception as e:\n            print(f\"âŒ ClaimUnderstanding Error: {e}\")\n            return {\"claim_id\": claim_id, \"claim_text\": claim_text, \"key_concepts\": []}\n\nclass EvidenceRetrievalAgent:\n    def __init__(self, search_tool):\n        self.search_tool = search_tool\n\n    def retrieve_evidence(self, parsed_claim: Dict[str, Any]) -> Dict[str, Any]:\n        text = parsed_claim.get(\"claim_text\", \"\")\n        concepts = parsed_claim.get(\"key_concepts\", [])\n        if isinstance(concepts, list): concepts = \" \".join(str(c) for c in concepts)\n        query = f\"{text} {concepts}\"\n        return {\"parsed_claim\": parsed_claim, \"candidate_evidence\": self.search_tool.search_appendices(query, top_k=4)}\n\nclass EvidenceAnalysisAgent:\n    def __init__(self, model):\n        self.model = model\n\n    def analyze(self, retrieval_bundle: Dict[str, Any]) -> Dict[str, Any]:\n        evidence = retrieval_bundle.get(\"candidate_evidence\", [])\n        if not evidence:\n            return {\"verdict\": \"unsupported\", \"explanation\": \"No evidence found.\", \"evidence_appendix_ids\": []}\n\n        ev_text = \"\".join([f\"\\\\n[Appendix {e['appendix_id']}]: {e['snippet']}...\" for e in evidence])\n        prompt = f'''Verify if claim is supported by evidence.\n        Claim: \"{retrieval_bundle['parsed_claim'].get('claim_text','')}\"\n        Evidence: {ev_text}\n        Output JSON: {{ \"verdict\": \"supported\"|\"unsupported\"|\"partially_supported\", \"evidence_appendix_ids\": [], \"explanation\": \"\" }}'''\n        \n        try:\n            resp = self.model.generate_content(prompt)\n            parsed = json.loads(resp.text)\n            if isinstance(parsed, list): parsed = parsed[0]\n            return parsed\n        except Exception as e:\n            print(f\"âŒ Analysis Error: {e}\")\n            return {\"verdict\": \"error\", \"explanation\": f\"Error: {e}\", \"evidence_appendix_ids\": []}\n\"\"\"\nwith open(\"/kaggle/working/qcb_evidence_agent/src/agents.py\", \"w\") as f:\n    f.write(agents_code)\n\n# 4. Create orchestrator_2.py\norch_code = \"\"\"\nfrom typing import Dict, Any\nfrom .agents import get_gemini_model, ClaimUnderstandingAgent, EvidenceRetrievalAgent, EvidenceAnalysisAgent\nfrom .tools import AppendixSearchTool\n\nclass ChatOrchestrator:\n    def __init__(self, data_dir: str):\n        self.data_dir = data_dir\n        self.model = get_gemini_model()\n        self.search_tool = AppendixSearchTool(f\"{data_dir}/data/appendices\")\n        self.understander = ClaimUnderstandingAgent(self.model)\n        self.retriever = EvidenceRetrievalAgent(self.search_tool)\n        self.analyzer = EvidenceAnalysisAgent(self.model)\n\n    def review_freeform_claim(self, claim_text: str) -> Dict[str, Any]:\n        parsed = self.understander.understand_claim(\"user_query\", claim_text)\n        context = self.retriever.retrieve_evidence(parsed)\n        result = self.analyzer.analyze(context)\n        return {\n            \"original_text\": claim_text,\n            \"verdict\": result.get(\"verdict\", \"error\"),\n            \"explanation\": result.get(\"explanation\", \"Analysis failed.\"),\n            \"evidence_snippets\": context.get(\"candidate_evidence\", [])\n        }\n\"\"\"\nwith open(\"/kaggle/working/qcb_evidence_agent/src/orchestrator_2.py\", \"w\") as f:\n    f.write(orch_code)\n\n# 5. Create tools.py & data_index.py\ndata_index_code = \"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nclass AppendixIndex:\n    def __init__(self, appendices_dir: str):\n        self.appendices_dir = Path(appendices_dir)\n        self.filepaths: List[Path] = []\n        self.texts: List[str] = []\n        self.appendix_ids: List[str] = []\n        self.vectorizer: TfidfVectorizer | None = None\n        self.tfidf_matrix = None\n\n    def load_appendices(self) -> None:\n        self.filepaths = sorted(list(self.appendices_dir.glob(\"**/*.txt\")))\n        for fp in self.filepaths:\n            self.texts.append(fp.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n            self.appendix_ids.append(fp.stem.split(\"_\")[1] if \"_\" in fp.stem else fp.stem)\n\n    def build_index(self) -> None:\n        if not self.texts:\n             self.vectorizer = TfidfVectorizer(max_features=100)\n             self.tfidf_matrix = self.vectorizer.fit_transform([\"dummy\"])\n             return\n        self.vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n        self.tfidf_matrix = self.vectorizer.fit_transform(self.texts)\n\n    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        if self.vectorizer is None or not self.texts: return []\n        q_vec = self.vectorizer.transform([query])\n        scores = (self.tfidf_matrix @ q_vec.T).toarray().ravel()\n        top_idx = np.argsort(scores)[::-1][:top_k]\n        results = []\n        for idx in top_idx:\n            if scores[idx] > 0:\n                results.append({\n                    \"appendix_id\": self.appendix_ids[idx],\n                    \"filename\": self.filepaths[idx].name,\n                    \"score\": float(scores[idx]),\n                    \"snippet\": self.texts[idx][:800]\n                })\n        return results\n\"\"\"\nwith open(\"/kaggle/working/qcb_evidence_agent/src/data_index.py\", \"w\") as f:\n    f.write(data_index_code)\n\ntools_code = \"\"\"\nfrom typing import List, Dict, Any\nfrom .data_index import AppendixIndex\n\nclass AppendixSearchTool:\n    def __init__(self, appendices_dir: str):\n        self.index = AppendixIndex(appendices_dir)\n        self.index.load_appendices()\n        self.index.build_index()\n\n    def search_appendices(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        return self.index.search(query_text, top_k=top_k)\n\"\"\"\nwith open(\"/kaggle/working/qcb_evidence_agent/src/tools.py\", \"w\") as f:\n    f.write(tools_code)\n\nprint(\"âœ… Codebase fully restored.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:45:42.157294Z","iopub.execute_input":"2025-11-30T07:45:42.157745Z","iopub.status.idle":"2025-11-30T07:45:42.172625Z","shell.execute_reply.started":"2025-11-30T07:45:42.157674Z","shell.execute_reply":"2025-11-30T07:45:42.171506Z"}},"outputs":[{"name":"stdout","text":"âœ… Codebase fully restored.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import zipfile\nimport os\n\nzip_path = \"/kaggle/working/qcb_capstone_data.zip\"\ndata_dir = \"/kaggle/working/qcb_evidence_agent/data\"\n\nif os.path.exists(zip_path):\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(data_dir)\n    print(\"âœ… Data restored.\")\nelse:\n    print(\"âš ï¸ ZIP file not found! Please re-upload 'qcb_capstone_data.zip' or check /kaggle/input/.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:45:48.159991Z","iopub.execute_input":"2025-11-30T07:45:48.160394Z","iopub.status.idle":"2025-11-30T07:45:48.210782Z","shell.execute_reply.started":"2025-11-30T07:45:48.160358Z","shell.execute_reply":"2025-11-30T07:45:48.209834Z"}},"outputs":[{"name":"stdout","text":"âœ… Data restored.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/working/qcb_evidence_agent\")\nfrom src.orchestrator_2 import ChatOrchestrator\n\ntry:\n    agent = ChatOrchestrator(\"/kaggle/working/qcb_evidence_agent\")\n    res = agent.review_freeform_claim(\"Participants liked the summer school.\")\n    print(f\"Verdict: {res['verdict']}\")\n    print(f\"Reasoning: {res['explanation']}\")\nexcept Exception as e:\n    print(f\"Test Failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:45:51.937718Z","iopub.execute_input":"2025-11-30T07:45:51.938062Z","iopub.status.idle":"2025-11-30T07:45:53.800382Z","shell.execute_reply.started":"2025-11-30T07:45:51.938038Z","shell.execute_reply":"2025-11-30T07:45:53.799120Z"}},"outputs":[{"name":"stdout","text":"Verdict: unsupported\nReasoning: The evidence provided indicates that participants felt the summer school experience shaped how they think about various aspects like cognitive overload, pacing, lab-computation connection, belonging, collaboration barriers, instructor clarity, mentoring support and interdisciplinarity. While these could be interpreted as related to liking the summer school, it does not directly support the claim that participants 'liked' it. The evidence focuses on how the summer school shaped their thinking, not necessarily their overall sentiment towards it.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import shutil\nimport os\nfrom IPython.display import FileLink\n\n# Define the source folder and the output zip name\nfolder_path = \"/kaggle/working/qcb_evidence_agent\"\noutput_filename = \"evidence_agent_full_project\"\nzip_path = f\"/kaggle/working/{output_filename}\"\n\n# Check if the folder exists before zipping\nif os.path.exists(folder_path):\n    print(\"Zipping project files...\")\n    shutil.make_archive(zip_path, 'zip', folder_path)\n    print(f\"âœ… Project successfully zipped: {output_filename}.zip\")\n    \n    # Generate the clickable link\n    print(\"\\nðŸ‘‡ CLICK HERE TO DOWNLOAD ðŸ‘‡\")\n    display(FileLink(f'{output_filename}.zip'))\nelse:\n    print(\"âŒ Error: The 'qcb_evidence_agent' folder does not exist. Please run the restoration scripts first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:46:48.548540Z","iopub.execute_input":"2025-11-30T07:46:48.549204Z","iopub.status.idle":"2025-11-30T07:46:48.621252Z","shell.execute_reply.started":"2025-11-30T07:46:48.549176Z","shell.execute_reply":"2025-11-30T07:46:48.620390Z"}},"outputs":[{"name":"stdout","text":"Zipping project files...\nâœ… Project successfully zipped: evidence_agent_full_project.zip\n\nðŸ‘‡ CLICK HERE TO DOWNLOAD ðŸ‘‡\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/evidence_agent_full_project.zip","text/html":"<a href='evidence_agent_full_project.zip' target='_blank'>evidence_agent_full_project.zip</a><br>"},"metadata":{}}],"execution_count":43}]}